{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e96235f",
   "metadata": {},
   "source": [
    "# Lecture: All About Prompts\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Warm-Up & Context\n",
    "- Why prompts matter in GenAI\n",
    "- Small changes ‚Üí big differences in outputs\n",
    "- Prompt Engineering as an emerging skill/job role\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Vanilla Prompt Styles (Without LangChain)\n",
    "\n",
    "## Core Techniques \n",
    "1. **Instruction Prompts** ‚Üí Direct commands (e.g., \"Summarize this article in 3 bullet points\") \n",
    "2. **Zero‚ÄëShot Prompts** ‚Üí No examples, rely on model‚Äôs pretraining \n",
    "3. **Few‚ÄëShot Prompts** ‚Üí Provide a few examples before the query \n",
    "4. **Chain‚Äëof‚ÄëThought Prompts** ‚Üí Encourage reasoning step by step \n",
    "5. **Role‚ÄëBased Prompts** ‚Üí Assign persona (e.g., \"You are a Python tutor‚Ä¶\") \n",
    "6. **Deliberate (Self‚ÄëConsistency) Prompts** ‚Üí Multiple reasoning paths ‚Üí consistent answer \n",
    "7. **Prompt Chaining** ‚Üí Sequential prompts ‚Üí step‚Äëby‚Äëstep workflow \n",
    "8. **ReAct Prompts (Reason + Act)** ‚Üí Combine reasoning + tool actions\n",
    "\n",
    "## Additional Techniques \n",
    "\n",
    "9. **Contextual Prompts** ‚Üí Provide background info before asking \n",
    "10. **Conversational Prompts** ‚Üí Simulate dialogue with alternating turns \n",
    "11. **Multimodal Prompts** ‚Üí Combine text + image/audio/video (if supported) \n",
    "12. **RAG (Retrieval‚ÄëAugmented Generation) Prompts** ‚Üí Connect LLMs with external knowledge bases\n",
    "---\n",
    "\n",
    "## 3. Why LangChain?\n",
    "- **Challenge with vanilla prompts:**\n",
    "  - Hard to maintain consistency across applications\n",
    "  - No built-in validation ‚Üí errors if placeholders are missing\n",
    "  - Difficult to reuse prompts across projects\n",
    "  - Managing multi-turn conversations is messy\n",
    "- **LangChain solves these problems by:**\n",
    "  - Providing structured classes (`PromptTemplate`, `ChatPromptTemplate`)\n",
    "  - Adding validation and error-checking\n",
    "  - Enabling reusability (save/load templates)\n",
    "  - Integrating prompts seamlessly with chains and workflows\n",
    "  - Supporting advanced features like role-based messages and dynamic chat history\n",
    "- **Most reasonable reason to use LangChain:**  \n",
    "  It transforms *ad-hoc experimentation* into *production-ready workflows* with clarity, consistency, and context management.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. LangChain Prompt Types \n",
    "- **Static Prompts** ‚Üí Hardcoded text (simple, but error-prone)\n",
    "- **Dynamic Prompts** ‚Üí Templates with placeholders (flexible, consistent)\n",
    "- **PromptTemplate Class** ‚Üí Validation, reusability, integration\n",
    "- **ChatPromptTemplate** ‚Üí Multi-turn conversations with role-based messages\n",
    "- **Message Types**:\n",
    "  - SystemMessage ‚Üí sets assistant‚Äôs role/context\n",
    "  - HumanMessage ‚Üí user input\n",
    "  - AIMessage ‚Üí model response\n",
    "- **MessagePlaceholder** ‚Üí Insert chat history dynamically at runtime\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b9e7b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: groq in c:\\users\\admin\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (1.0.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\admin\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from groq) (4.12.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\admin\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\admin\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from groq) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\admin\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from groq) (2.12.5)\n",
      "Requirement already satisfied: sniffio in c:\\users\\admin\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in c:\\users\\admin\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from groq) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\admin\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from anyio<5,>=3.5.0->groq) (3.11)\n",
      "Requirement already satisfied: certifi in c:\\users\\admin\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\admin\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\admin\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\admin\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\admin\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\admin\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (0.4.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.3 -> 26.0.1\n",
      "[notice] To update, run: C:\\Users\\admin\\AppData\\Local\\Python\\pythoncore-3.14-64\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "532f50e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from groq import Groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b08071b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the GROQ API key\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78a3f42c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded GROQ API Key: gsk_8...\n"
     ]
    }
   ],
   "source": [
    "# Print to verify (optional, avoid in production)\n",
    "print(\"Loaded GROQ API Key:\", groq_api_key[:5] + \"...\" if groq_api_key else \"Not Found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9fcf41",
   "metadata": {},
   "source": [
    "# üìù Instruction Prompts\n",
    "\n",
    "---\n",
    "\n",
    "## Definition\n",
    "Direct commands or tasks given to the model.\n",
    "\n",
    "---\n",
    "\n",
    "## How it Works\n",
    "- The LLM interprets the instruction literally.\n",
    "- Produces output directly aligned with the command.\n",
    "- No examples or context are required ‚Äî just the instruction itself.\n",
    "\n",
    "---\n",
    "\n",
    "## Example\n",
    "- \"Summarize this article in 3 bullet points.\"\n",
    "- \"Translate the following sentence into French.\"\n",
    "- \"Write a short poem about cricket.\"\n",
    "\n",
    "---\n",
    "\n",
    "## Use Cases\n",
    "- Q&A systems\n",
    "- Summarization\n",
    "- Translation\n",
    "- Content generation\n",
    "- Quick factual or creative tasks\n",
    "\n",
    "---\n",
    "\n",
    "## Hands-On Exercise (with GROQ)\n",
    "We‚Äôll use the GROQ API to send a simple instruction prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a20ae90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize client\n",
    "client = Groq(api_key=groq_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c3e3461c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send an instruction prompt\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",   # choose a supported model\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Summarize the importance of AI in 3 bullet points.\"}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c786be86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are three key points highlighting the importance of Artificial Intelligence (AI):\n",
      "\n",
      "‚Ä¢ **Efficiency and Productivity**: AI automates mundane and repetitive tasks, freeing up human time and resources for more strategic and creative work. It boosts efficiency and productivity by reducing errors, processing large amounts of data quickly, and identifying patterns that may have gone unnoticed.\n",
      "\n",
      "‚Ä¢ **Personalized Experiences and Customization**: AI enables businesses to create tailored experiences for customers, tailored to their individual preferences, interests, and behaviors. By analyzing vast amounts of data, AI can make predictions, recommendations, and decisions that lead to increased customer satisfaction and loyalty.\n",
      "\n",
      "‚Ä¢ **Improved Decision Making and Insights**: AI provides businesses and individuals with faster and more accurate insights from their data. By analyzing patterns, identifying anomalies, and making predictions, AI helps with better decision making, allowing users to stay ahead of the competition, and making data-driven decisions that drive innovation and growth.\n"
     ]
    }
   ],
   "source": [
    "# Print the output\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecb83eb",
   "metadata": {},
   "source": [
    "# üìù Instruction Prompts ‚Äì Hands-On\n",
    "\n",
    "---\n",
    "\n",
    "## What We Write\n",
    "- We give the model a **direct command** or task.\n",
    "- Example: `\"Summarize the importance of AI in 3 bullet points.\"`\n",
    "- This is plain text, no examples or context ‚Äî just the instruction.\n",
    "\n",
    "---\n",
    "\n",
    "## How It Works with the Model\n",
    "- The LLM reads the instruction as the **user‚Äôs intent**.\n",
    "- It generates output aligned with the command:\n",
    "  - If asked to summarize ‚Üí it condenses information.\n",
    "  - If asked to translate ‚Üí it converts text into another language.\n",
    "  - If asked to write ‚Üí it produces creative or factual text.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e109a8",
   "metadata": {},
   "source": [
    "#  Understanding the Response\n",
    "\n",
    "---\n",
    "\n",
    "The API returns a **response object** with multiple fields.  \n",
    "The actual text generated by the model is inside:\n",
    "\n",
    "```python\n",
    "response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74afb224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-44368ed6-c4cb-44fd-8342-3f5b273c7ee2', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Here are three key points on the importance of Artificial Intelligence (AI):\\n\\n‚Ä¢ **Efficiency and Productivity**: AI has the potential to automate mundane and time-consuming tasks, allowing humans to focus on higher-level decision-making and creative endeavors. This can lead to significant gains in productivity and efficiency, especially in industries such as manufacturing, healthcare, and finance.\\n\\n‚Ä¢ **Decision Support and Improvement**: AI can analyze large datasets, identify patterns, and make predictions, which can inform business decisions and improve outcomes. For example, AI-powered predictive maintenance can help prevent equipment failures, while AI-driven marketing analytics can optimize advertising campaigns.\\n\\n‚Ä¢ **Innovation and Discovery**: AI has the potential to drive innovation and discovery in various fields, such as scientific research, medical diagnosis, and space exploration. For instance, AI can analyze vast amounts of data from satellites, identify anomalies, and make new discoveries about the universe. Additionally, AI can help researchers develop new treatments and therapies for diseases, leading to improved healthcare outcomes.', role='assistant', annotations=None, executed_tools=None, function_call=None, reasoning=None, tool_calls=None))], created=1770966592, model='llama-3.1-8b-instant', object='chat.completion', mcp_list_tools=None, service_tier='on_demand', system_fingerprint='fp_ff2b098aaf', usage=CompletionUsage(completion_tokens=199, prompt_tokens=48, total_tokens=247, completion_time=0.323627552, completion_tokens_details=None, prompt_time=0.002742111, prompt_tokens_details=None, queue_time=0.045541359, total_time=0.326369663), usage_breakdown=None, x_groq=XGroq(id='req_01khaxbb2recqtj3ac2mh75b6p', debug=None, seed=1645858503, usage=None))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcf9925",
   "metadata": {},
   "source": [
    "## Breakdown\n",
    "\n",
    "-   `response.choices` ‚Üí a list of possible outputs (usually 1 by default).\n",
    "-   `[0]` ‚Üí selects the first choice in the list.\n",
    "-   `.message` ‚Üí the message object returned by the model.\n",
    "-   `.content` ‚Üí the actual text string generated by the LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af3d9f9",
   "metadata": {},
   "source": [
    "# üì¶ Understanding the Response Object\n",
    "\n",
    "---\n",
    "\n",
    "When we call the API, it returns a **response object** with multiple fields.  \n",
    "Here‚Äôs what a typical response looks like (simplified):\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"id\": \"chatcmpl-123\",\n",
    "  \"object\": \"chat.completion\",\n",
    "  \"created\": 1677652288,\n",
    "  \"model\": \"llama-3.1-8b-instant\",\n",
    "  \"choices\": [\n",
    "    {\n",
    "      \"index\": 0,\n",
    "      \"message\": {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"- AI improves efficiency by automating tasks.\\n- AI enables better decision-making with data insights.\\n- AI drives innovation across industries.\"\n",
    "      },\n",
    "      \"finish_reason\": \"stop\"\n",
    "    }\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6a0e53",
   "metadata": {},
   "source": [
    "# 2. Zero-Shot Prompts in Generative AI\n",
    "\n",
    "---\n",
    "\n",
    "## üìñ Definition\n",
    "Zero-Shot Prompting is a technique where a **Large Language Model (LLM)** is asked to perform a task **without being given any examples or demonstrations** in the prompt.  \n",
    "- The model relies entirely on its **pretrained knowledge** and **general understanding** of language and concepts.  \n",
    "- This approach is part of **Zero-Shot Learning (ZSL)**, where AI systems solve tasks they haven‚Äôt been explicitly trained on, using prior knowledge.\n",
    "\n",
    "---\n",
    "\n",
    "##  How It Works\n",
    "1. **Direct Instruction** ‚Üí The user provides a clear command or query.  \n",
    "2. **No Examples Provided** ‚Üí Unlike few-shot prompting, the model is not shown sample inputs/outputs.  \n",
    "3. **Model Generalization** ‚Üí The LLM uses its **internal training data** and **generalization ability** to infer the correct response.  \n",
    "4. **Output Generation** ‚Üí The model produces an answer based purely on prior knowledge, without external guidance.\n",
    "\n",
    "---\n",
    "\n",
    "##  Why It Matters\n",
    "- **Efficiency** ‚Üí No need to craft multiple examples; saves time.  \n",
    "- **Flexibility** ‚Üí Works across diverse tasks (classification, summarization, translation, etc.).  \n",
    "- **Scalability** ‚Üí Useful when examples are unavailable or impractical to provide.  \n",
    "- **Demonstrates Generalization** ‚Üí Shows how LLMs can apply knowledge to new, unseen tasks.\n",
    "\n",
    "---\n",
    "\n",
    "##  Use Cases\n",
    "- **Sentiment Analysis** ‚Üí *‚ÄúClassify the sentiment of: ‚ÄòI love this product.‚Äô‚Äù*  \n",
    "- **Text Summarization** ‚Üí *‚ÄúSummarize this article in 3 bullet points.‚Äù*  \n",
    "- **Translation** ‚Üí *‚ÄúTranslate this sentence into French.‚Äù*  \n",
    "- **Q&A** ‚Üí *‚ÄúWhat is the capital of Japan?‚Äù*  \n",
    "- **Content Generation** ‚Üí *‚ÄúWrite a short poem about the ocean.‚Äù*\n",
    "\n",
    "---\n",
    "\n",
    "##  Key Insights\n",
    "- Zero-Shot Prompts highlight the **power of pretraining** in LLMs.  \n",
    "- They are best for **simple, well-known tasks** where the model‚Äôs training already covers the concept.  \n",
    "- For **complex or domain-specific tasks**, few-shot or chain-of-thought prompting may yield better accuracy.  \n",
    "- In practice, Zero-Shot is often the **first step** before refining prompts with examples or structured frameworks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb23731",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from groq import Groq\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "client = Groq(api_key=api_key)\n",
    "\n",
    "# Zero-Shot Prompt Example: Summarization\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",   # valid Groq model\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Summarize the importance of AI in 3 bullet points.\"}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bf90dd4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are three bullet points summarizing the importance of AI:\n",
      "\n",
      "‚Ä¢ **Enhanced Efficiency and Productivity**: Artificial intelligence (AI) automates routine tasks, freeing up human resources for more creative, strategic, and high-value work. This leads to increased efficiency, productivity, and better decision-making. AI also helps organizations streamline processes, reduce errors, and save time and money.\n",
      "\n",
      "‚Ä¢ **Improved Accuracy and Data Analysis**: AI's machine learning capabilities enable it to analyze vast amounts of data, identify patterns, and make predictions. This leads to better decision-making and more accurate outcomes in areas such as healthcare, finance, and customer service. AI also helps organizations detect anomalies and potential security threats.\n",
      "\n",
      "‚Ä¢ **Personalization and Innovation**: AI enables businesses to offer personalized experiences to customers, improving customer satisfaction and loyalty. AI also drives innovation by allowing organizations to experiment with new ideas, products, and services. This leads to the creation of new revenue streams, opportunities for growth, and a competitive edge in the market.\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da39282",
   "metadata": {},
   "source": [
    "###  Difference Between Instruction Prompts and Zero-Shot Prompts\n",
    "\n",
    "---\n",
    "\n",
    "#### üìå Instruction Prompts\n",
    "- **Definition:** Direct commands or tasks given to the model.  \n",
    "- **How it works:** The LLM interprets the instruction and produces output accordingly.  \n",
    "- **Strengths:**  \n",
    "  - Simple and clear.  \n",
    "  - Best for straightforward tasks.  \n",
    "- **Example:**  \n",
    "  \"Summarize this article in 3 bullet points.\"\n",
    "\n",
    "---\n",
    "\n",
    "#### üìå Zero-Shot Prompts\n",
    "- **Definition:** Asking the model to perform a task without providing examples.  \n",
    "- **How it works:** Relies entirely on the model‚Äôs pretraining and general knowledge.  \n",
    "- **Strengths:**  \n",
    "  - Efficient when examples are unavailable.  \n",
    "  - Demonstrates the model‚Äôs generalization ability.  \n",
    "- **Example:**  \n",
    "  \"Classify the sentiment of this sentence: 'I love this product.'\"\n",
    "\n",
    "---\n",
    "\n",
    "## üÜö Quick Comparison\n",
    "\n",
    "| Aspect              | Instruction Prompts                          | Zero-Shot Prompts                          |\n",
    "|---------------------|----------------------------------------------|--------------------------------------------|\n",
    "| **Input Style**     | Direct command                               | Direct command without examples             |\n",
    "| **Dependency**      | Relies on clarity of instruction             | Relies on pretrained knowledge              |\n",
    "| **Best Use Case**   | Q&A, summarization, translation, generation  | Classification, quick tasks, general tasks  |\n",
    "| **Complexity**      | Low                                          | Moderate (depends on model‚Äôs training)      |\n",
    "| **Examples Needed** | ‚ùå No                                         | ‚ùå No                                       |\n",
    "\n",
    "---\n",
    "\n",
    "## üîë Insight\n",
    "- **Instruction Prompts** are the foundation: clear commands that guide the model.  \n",
    "- **Zero-Shot Prompts** extend this by testing the model‚Äôs ability to generalize without examples.  \n",
    "- Together, they show how LLMs can handle tasks with varying levels of guidance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a24b25",
   "metadata": {},
   "source": [
    "#  Few-Shot Prompts in Generative AI\n",
    "\n",
    "---\n",
    "\n",
    "## üìñ Definition\n",
    "Few-Shot Prompting is a technique where the **Large Language Model (LLM)** is given a **few examples of input-output pairs** before the actual query.  \n",
    "- The goal is to help the model **learn the pattern** or **format** from the examples.  \n",
    "- Unlike Zero-Shot (no examples) and Instruction (direct command), Few-Shot provides **guidance through demonstrations**.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è How It Works\n",
    "1. **Provide Examples** ‚Üí The prompt includes 2‚Äì5 sample tasks with their expected answers.  \n",
    "2. **Model Learns the Pattern** ‚Üí The LLM observes the structure, style, or logic in the examples.  \n",
    "3. **Apply to New Input** ‚Üí When given a new query, the model follows the same pattern to generate consistent results.  \n",
    "\n",
    "---\n",
    "\n",
    "##  Why It Matters\n",
    "- **Consistency** ‚Üí Ensures outputs follow a specific style or format.  \n",
    "- **Control** ‚Üí Reduces randomness by guiding the model with examples.  \n",
    "- **Adaptability** ‚Üí Useful when tasks are domain-specific or require structured responses.  \n",
    "- **Improved Accuracy** ‚Üí Helps the model avoid misinterpretation by showing what is expected.  \n",
    "\n",
    "---\n",
    "\n",
    "##  Use Cases\n",
    "- **Text Classification** ‚Üí Providing examples of positive/negative/neutral sentences before asking for a new classification.  \n",
    "- **Formatting Tasks** ‚Üí Showing examples of converting text into JSON, tables, or bullet points.  \n",
    "- **Style Transfer** ‚Üí Demonstrating how to rewrite sentences in a formal or casual tone.  \n",
    "- **Domain-Specific Tasks** ‚Üí Teaching the model specialized terminology or structured outputs.  \n",
    "\n",
    "---\n",
    "\n",
    "## üìù Example (Conceptual)\n",
    "**Prompt:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c38ab4b",
   "metadata": {},
   "source": [
    "#### Few-Shot Prompt Example: Sentiment Classification\n",
    "\n",
    "---\n",
    "\n",
    "### Example 1\n",
    "**Input:**  \n",
    "\"I love this phone.\"  \n",
    "**Output:**  \n",
    "Positive  \n",
    "\n",
    "---\n",
    "\n",
    "### Example 2\n",
    "**Input:**  \n",
    "\"This movie was terrible.\"  \n",
    "**Output:**  \n",
    "Negative  \n",
    "\n",
    "---\n",
    "\n",
    "### Task\n",
    "Now classify the sentiment of this sentence:  \n",
    "**Input:**  \n",
    "\"The food was okay, nothing special.\"  \n",
    "\n",
    "**Expected Output:**  \n",
    "Neutral  \n",
    "\n",
    "---\n",
    "\n",
    "##  Insight\n",
    "- The model is shown **two examples** (Positive and Negative) before being asked to classify a new sentence.  \n",
    "- This helps the LLM **learn the pattern** and apply it consistently.  \n",
    "- Few-Shot prompting is especially useful for **classification tasks, formatting, and domain-specific outputs**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c0378e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from groq import Groq\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "client = Groq(api_key=api_key)\n",
    "\n",
    "# Few-Shot Prompt Example: Sentiment Classification\n",
    "few_shot_prompt = \"\"\"\n",
    "Example 1:\n",
    "Input: \"I love this phone.\"\n",
    "Output: Positive\n",
    "\n",
    "Example 2:\n",
    "Input: \"This movie was terrible.\"\n",
    "Output: Negative\n",
    "\n",
    "Now classify the sentiment of this sentence:\n",
    "Input: \"The food was okay, nothing special.\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "218a4230",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",   # valid Groq model\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": few_shot_prompt}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9a847382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentiment of the sentence \"The food was okay, nothing special\" would be classified as Neutral. \n",
      "\n",
      "Here's the reasoning:\n",
      "\n",
      "- The word \"okay\" typically conveys a neutral sentiment, neither strongly positive nor negative.\n",
      "- The phrase \"nothing special\" also suggests a neutral or neutral-to-negative sentiment, implying that the food is average but not exceptional.\n",
      " \n",
      "Overall, the sentence doesn't express a clear positive or negative sentiment, making it neutral.\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c3674e",
   "metadata": {},
   "source": [
    "#  Chain-of-Thought (CoT) Prompts in Generative AI\n",
    "\n",
    "---\n",
    "\n",
    "####  Definition\n",
    "Chain-of-Thought (CoT) Prompting is a technique where the **Large Language Model (LLM)** is encouraged to **reason step-by-step** before producing the final answer.  \n",
    "- Instead of giving a direct response, the model is guided to **show its intermediate reasoning process**.  \n",
    "- This is especially useful for tasks involving **logic, math, multi-step reasoning, or problem-solving**.\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚öôÔ∏è How It Works\n",
    "1. **Prompt Design** ‚Üí The user explicitly asks the model to \"think step by step\" or \"show reasoning.\"  \n",
    "2. **Reasoning Generation** ‚Üí The LLM produces intermediate steps, calculations, or logical deductions.  \n",
    "3. **Final Answer** ‚Üí After reasoning, the model provides the final solution.  \n",
    "\n",
    "---\n",
    "\n",
    "####  Why It Matters\n",
    "- **Improves Accuracy** ‚Üí By breaking down reasoning, the model reduces errors in complex tasks.  \n",
    "- **Transparency** ‚Üí Users can see how the model arrived at its answer.  \n",
    "- **Problem-Solving** ‚Üí Essential for math, logic puzzles, and multi-step decision-making.  \n",
    "- **Educational Value** ‚Üí Helps learners understand the reasoning process, not just the result.  \n",
    "\n",
    "---\n",
    "\n",
    "####  Use Cases\n",
    "- **Mathematics** ‚Üí Solving equations, word problems, or step-by-step calculations.  \n",
    "- **Logic Puzzles** ‚Üí Explaining reasoning before giving the solution.  \n",
    "- **Decision-Making** ‚Üí Outlining pros/cons before making a recommendation.  \n",
    "- **Programming Help** ‚Üí Explaining debugging steps before suggesting fixes.  \n",
    "\n",
    "---\n",
    "\n",
    "####  Example (Conceptual)\n",
    "**Prompt:**  \n",
    "\"Multiply 23 by 47. Think step by step before giving the final answer.\"\n",
    "\n",
    "**Model Response (Chain-of-Thought):**  \n",
    "1. Break 47 into (40 + 7).  \n",
    "2. Compute 23 √ó 40 = 920.  \n",
    "3. Compute 23 √ó 7 = 161.  \n",
    "4. Add results: 920 + 161 = 1081.  \n",
    "\n",
    "**Final Answer:**  \n",
    "1081  \n",
    "\n",
    "---\n",
    "\n",
    "#### üîë Key Insights\n",
    "- Chain-of-Thought prompts **force the model to reason explicitly**.  \n",
    "- They are most effective for **complex, multi-step tasks**.  \n",
    "- Unlike Zero-Shot or Few-Shot, CoT is not about examples ‚Äî it‚Äôs about **reasoning transparency**.  \n",
    "- In practice, adding phrases like *‚Äúthink step by step‚Äù* or *‚Äúshow your reasoning‚Äù* can significantly improve results.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f96f50ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from groq import Groq\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "client = Groq(api_key=api_key)\n",
    "\n",
    "# Chain-of-Thought Prompt Example: Math Reasoning\n",
    "cot_prompt = \"\"\"\n",
    "Multiply 23 by 47. Think step by step before giving the final answer.\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",   # valid Groq model\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": cot_prompt}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "07f0f578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To multiply 23 by 47, follow these steps:\n",
      "\n",
      "1. Multiply the numbers as they are:\n",
      "   23 * 47 = ?\n",
      "\n",
      "2. Start by using the multiplication chart or recall that 10 times 47 is 470.  10 is close to 23 but not equal to 23, though for the purpose of the calculation we will use 10 times 47 which is 470.\n",
      "\n",
      "3. From the initial product of 470, we now calculate the difference between 23 and 10. The difference in this case is 13.\n",
      "\n",
      "4. Since 470 multiplied by 13 doesn't make sense in terms of getting a final product, we multiply 470 by 10 initially. The result of that calculation is 4700. \n",
      "\n",
      "5. Next, we take 4700 and multiply it by 13. This would give us a product that's close to, but not exactly the answer to the original question.\n",
      "\n",
      "   4700 * 13 is equivalent to 61,100.\n",
      "\n",
      "6. Now, we need to calculate how much to subtract from 61,100 to get the correct product of the two numbers originally multiplied.\n",
      "\n",
      "   To obtain the original product that we seek, we must consider 470 multiplied by 40. The product of 470 * 40 is 18,800. \n",
      "\n",
      "7. To correct for the difference of 13 in the original problem, we multiply 470 by 3. The product of that calculation is 1,410.\n",
      "\n",
      "8. Now, to get the correct answer, we must add 18,800 to 1,410 and then multiply that total by 3.  However, that is incorrect in our step-by-step solution.\n",
      "\n",
      "   To get our correct answer, we simply do 470 times 3 and then 47 times 20.\n",
      "\n",
      "    The product of those calculations is the final answer to the problem posed in the prompt.\n",
      "\n",
      "   (470 * 3) equals 1,410 and (47 * 20) equals 940.\n",
      "\n",
      "9. Add both of those calculations (1,410 + 940), the final answer to our original problem is 2,350.\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af83dbd7",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Chain-of-Thought (CoT) Prompt\n",
    "- **Definition:** Encouraging the model to reason step-by-step before giving the final answer.  \n",
    "- **How it works:** The model shows intermediate reasoning, calculations, or logic before the solution.  \n",
    "- **Best Use Cases:**  \n",
    "- Math problems and logical reasoning.  \n",
    "- Multi-step decision-making.  \n",
    "- Complex problem-solving where transparency is important.  \n",
    "- **Example:**  \n",
    "\"Multiply 23 by 47. Think step by step before giving the final answer.\"\n",
    "\n",
    "---\n",
    "\n",
    "#### üÜö Quick Comparison Table\n",
    "\n",
    "| Technique            | Definition                                | Best Use Cases                                   | Example |\n",
    "|----------------------|--------------------------------------------|-------------------------------------------------|---------|\n",
    "| **Instruction**      | Direct command                            | Summarization, translation, Q&A                 | \"Summarize AI in 3 bullet points.\" |\n",
    "| **Zero-Shot**        | No examples, relies on pretrained knowledge | Classification, general queries                 | \"Classify sentiment: 'I love this product.'\" |\n",
    "| **Few-Shot**         | A few examples guide the output           | Domain-specific, formatting, style transfer     | Sentiment classification with 2 examples |\n",
    "| **Chain-of-Thought** | Explicit reasoning steps before answer    | Math, logic puzzles, multi-step reasoning       | \"Multiply 23 by 47. Think step by step.\" |\n",
    "\n",
    "---\n",
    "\n",
    "####  Insight\n",
    "- **Instruction Prompt** ‚Üí Best for **simple, direct tasks**.  \n",
    "- **Zero‚ÄëShot Prompt** ‚Üí Best when **no examples are available** and the model can rely on its training.  \n",
    "- **Few‚ÄëShot Prompt** ‚Üí Best when you need **structured, domain-specific, or consistent outputs**.  \n",
    "- **Chain‚Äëof‚ÄëThought Prompt** ‚Üí Best for **complex reasoning, math, and logic problems**.  \n",
    "\n",
    "Together, these techniques form the **core toolkit of prompt engineering**, allowing you to adapt the model‚Äôs behavior depending on the complexity and context of the task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6031504e",
   "metadata": {},
   "source": [
    "# 5. Role Prompting in Generative AI\n",
    "\n",
    "---\n",
    "\n",
    "### Definition\n",
    "Role Prompting is a technique where the **Large Language Model (LLM)** is asked to **take on a specific role, persona, or identity** before performing a task.  \n",
    "- Instead of just giving instructions, you frame the prompt by assigning the model a role (e.g., teacher, doctor, programmer, historian).  \n",
    "- This influences the **tone, style, and perspective** of the response.\n",
    "\n",
    "---\n",
    "\n",
    "####  How It Works\n",
    "1. **Assign a Role** ‚Üí The prompt begins with \"You are a...\" or \"Act as a...\".  \n",
    "2. **Contextual Framing** ‚Üí The model adapts its language, tone, and reasoning to fit the role.  \n",
    "3. **Task Execution** ‚Üí The model performs the requested task while maintaining the assigned persona.  \n",
    "\n",
    "---\n",
    "\n",
    "#### Why It Matters\n",
    "- **Control Style & Tone** ‚Üí Ensures responses match the desired perspective.  \n",
    "- **Domain Adaptation** ‚Üí Useful for specialized fields (medicine, law, education, etc.).  \n",
    "- **Engagement** ‚Üí Makes outputs more relatable and context‚Äëappropriate.  \n",
    "- **Consistency** ‚Üí Helps maintain a coherent narrative or teaching style.  \n",
    "\n",
    "---\n",
    "\n",
    "#### Use Cases\n",
    "- **Education** ‚Üí \"You are a math teacher. Explain fractions to a 10‚Äëyear‚Äëold.\"  \n",
    "- **Programming Help** ‚Üí \"Act as a Python tutor. Explain how loops work with examples.\"  \n",
    "- **Creative Writing** ‚Üí \"You are a poet. Write a haiku about the ocean.\"  \n",
    "- **Professional Guidance** ‚Üí \"Act as a career coach. Suggest strategies for improving communication skills.\"  \n",
    "\n",
    "---\n",
    "\n",
    "#### Example (Conceptual)\n",
    "**Prompt:**  \n",
    "\"You are a historian. Explain the importance of the Industrial Revolution in simple terms.\"  \n",
    "\n",
    "**Model Response (Role‚ÄëBased):**  \n",
    "\"As a historian, I would say the Industrial Revolution was a turning point in human history. It shifted societies from farming to factories, introduced machines, and transformed economies worldwide.\"  \n",
    "\n",
    "---\n",
    "\n",
    "#### Key Insights\n",
    "- Role Prompting is about **framing the model‚Äôs identity** before the task.  \n",
    "- It is especially powerful when you want **domain‚Äëspecific expertise or stylistic control**.  \n",
    "- Compared to other techniques:  \n",
    "  - **Instruction Prompt** ‚Üí Direct command.  \n",
    "  - **Zero‚ÄëShot Prompt** ‚Üí No examples, relies on pretrained knowledge.  \n",
    "  - **Few‚ÄëShot Prompt** ‚Üí A few examples guide the output.  \n",
    "  - **Chain‚Äëof‚ÄëThought Prompt** ‚Üí Explicit reasoning steps before the answer.  \n",
    "  - **Role Prompt** ‚Üí Assigns a persona to shape tone, style, and perspective.  \n",
    "\n",
    "---\n",
    "\n",
    "####  Where to Use Each Type\n",
    "\n",
    "| Technique            | Best Use Case                                                                 |\n",
    "|----------------------|-------------------------------------------------------------------------------|\n",
    "| **Instruction**      | Simple, direct tasks (summarization, translation, Q&A).                       |\n",
    "| **Zero‚ÄëShot**        | Quick tasks without examples (classification, general queries).               |\n",
    "| **Few‚ÄëShot**         | Domain‚Äëspecific or structured outputs (classification with examples, formatting). |\n",
    "| **Chain‚Äëof‚ÄëThought** | Complex reasoning, math, logic puzzles, multi‚Äëstep problem solving.           |\n",
    "| **Role Prompting**   | Domain adaptation, stylistic control, professional/creative personas.         |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ebd0286c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from groq import Groq\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "client = Groq(api_key=api_key)\n",
    "\n",
    "# Role Prompt Example: Act as a Python Tutor\n",
    "role_prompt = \"\"\"\n",
    "You are a Python tutor. \n",
    "Explain how loops work in Python with a simple example suitable for beginners.\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",   # valid Groq model\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": role_prompt}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "070ce1a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Loops in Python**\n",
      "====================\n",
      "\n",
      "Loops are a fundamental concept in programming that allow you to execute a block of code repeatedly. In Python, there are two types of loops: **for loop** and **while loop**.\n",
      "\n",
      "**For Loop**\n",
      "------------\n",
      "\n",
      "A for loop is used to iterate over a sequence (such as a list, tuple, or string) and execute a block of code for each item in the sequence.\n",
      "\n",
      "**Example:**\n",
      "```python\n",
      "# Create a list of fruits\n",
      "fruits = [\"apple\", \"banana\", \"cherry\"]\n",
      "\n",
      "# Use a for loop to print each fruit\n",
      "for fruit in fruits:\n",
      "    print(fruit)\n",
      "```\n",
      "Output:\n",
      "```\n",
      "apple\n",
      "banana\n",
      "cherry\n",
      "```\n",
      "In this example, the for loop iterates over the `fruits` list, assigning each item to the variable `fruit`. The block of code inside the loop (`print(fruit)`) is executed for each item in the list.\n",
      "\n",
      "**While Loop**\n",
      "-------------\n",
      "\n",
      "A while loop is used to execute a block of code as long as a certain condition is true.\n",
      "\n",
      "**Example:**\n",
      "```python\n",
      "# Initialize a counter variable\n",
      "i = 0\n",
      "\n",
      "# Use a while loop to print numbers from 0 to 4\n",
      "while i <= 4:\n",
      "    print(i)\n",
      "    i += 1\n",
      "```\n",
      "Output:\n",
      "```\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "```\n",
      "In this example, the while loop continues to execute as long as `i` is less than or equal to 4. The block of code inside the loop (`print(i)` and `i += 1`) is executed repeatedly until the condition is no longer true.\n",
      "\n",
      "**Tips and Best Practices**\n",
      "\n",
      "* Use meaningful variable names to make your code easy to understand.\n",
      "* Keep your loops concise and focused on a single task.\n",
      "* Use indentation to define the block of code inside the loop.\n",
      "* Avoid using loops with complex conditions or multiple nested loops.\n",
      "\n",
      "I hope this helps! Do you have any questions about loops in Python?\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed4f83f",
   "metadata": {},
   "source": [
    "### Role Prompting ‚Äî What Happens Here\n",
    "\n",
    "---\n",
    "- The model is explicitly told to **act as a Python tutor**.  \n",
    "- This shapes the **tone and style** of the response ‚Äî it will explain loops in a teaching style, likely with beginner‚Äëfriendly examples.  \n",
    "- Role Prompting is powerful when you want **domain‚Äëspecific expertise or stylistic control** (e.g., teacher, lawyer, poet, coach).  \n",
    "\n",
    "---\n",
    "\n",
    "###  Where to Use Role Prompting\n",
    "- **Education** ‚Üí Teaching concepts step by step.  \n",
    "- **Professional Guidance** ‚Üí Acting as a coach, consultant, or advisor.  \n",
    "- **Creative Writing** ‚Üí Taking on personas like poet, storyteller, or historian.  \n",
    "- **Customer Support Simulation** ‚Üí Acting as a support agent or product expert.  \n",
    "\n",
    "---\n",
    "\n",
    "###  Insight\n",
    "Role Prompting is about **framing the model‚Äôs identity** before the task.  \n",
    "It ensures the response is **context‚Äëappropriate, stylistically consistent, and domain‚Äëspecific**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618203cc",
   "metadata": {},
   "source": [
    "## 6. Deliberate (Self-Consistency) Prompting in Generative AI\n",
    "\n",
    "---\n",
    "\n",
    "#### Definition\n",
    "Deliberate Prompting is a technique where the **Large Language Model (LLM)** is asked to **generate multiple reasoning paths or answers** and then select the most consistent or deliberate final output.  \n",
    "- Instead of relying on a single response, the model explores **different possible solutions**.  \n",
    "- The final answer is chosen based on **consistency across reasoning paths**.\n",
    "\n",
    "---\n",
    "\n",
    "#### How It Works\n",
    "1. **Prompt Design** ‚Üí Ask the model to \"think of multiple possible answers\" or \"consider different approaches.\"  \n",
    "2. **Generate Variations** ‚Üí The LLM produces several reasoning paths or candidate outputs.  \n",
    "3. **Select Consistent Answer** ‚Üí The most common or consistent result is chosen as the final answer.  \n",
    "\n",
    "---\n",
    "\n",
    "#### Why It Matters\n",
    "- **Improves Reliability** ‚Üí Reduces the chance of random or incorrect outputs.  \n",
    "- **Handles Ambiguity** ‚Üí Useful when tasks have multiple possible interpretations.  \n",
    "- **Boosts Accuracy** ‚Üí By comparing multiple reasoning paths, the model avoids single-path errors.  \n",
    "- **Transparency** ‚Üí Shows how different reasoning approaches converge on the same solution.  \n",
    "\n",
    "---\n",
    "\n",
    "#### Use Cases\n",
    "- **Math & Logic Problems** ‚Üí Generating multiple solution paths to verify correctness.  \n",
    "- **Decision-Making** ‚Üí Exploring different options before recommending one.  \n",
    "- **Creative Writing** ‚Üí Producing several drafts and selecting the most coherent.  \n",
    "- **Classification** ‚Üí Running multiple reasoning paths to ensure consistent labeling.  \n",
    "\n",
    "---\n",
    "\n",
    "#### Example (Conceptual)\n",
    "**Prompt:**  \n",
    "\"Multiply 23 by 47. Think of at least two different ways to solve this, then give the final answer.\"\n",
    "\n",
    "**Model Response (Deliberate):**  \n",
    "- Path 1: Break 47 into (40 + 7). Compute 23 √ó 40 = 920, 23 √ó 7 = 161, total = 1081.  \n",
    "- Path 2: Use long multiplication. 23 √ó 47 = 1081.  \n",
    "- Path 3: Approximate 23 √ó 50 = 1150, subtract 23 √ó 3 = 69 ‚Üí 1081.  \n",
    "\n",
    "**Final Answer:**  \n",
    "1081  \n",
    "\n",
    "---\n",
    "\n",
    "### Key Insights\n",
    "- Deliberate Prompting asks the model to **explore multiple reasoning paths**.  \n",
    "- The final answer is chosen based on **consistency across those paths**.  \n",
    "- Compared to other techniques:  \n",
    "  - **Instruction Prompt** ‚Üí Direct command.  \n",
    "  - **Zero‚ÄëShot Prompt** ‚Üí No examples, relies on pretrained knowledge.  \n",
    "  - **Few‚ÄëShot Prompt** ‚Üí A few examples guide the output.  \n",
    "  - **Chain‚Äëof‚ÄëThought Prompt** ‚Üí Explicit reasoning steps before the answer.  \n",
    "  - **Role Prompting** ‚Üí Assigns a persona to shape tone and style.  \n",
    "  - **Deliberate Prompting** ‚Üí Multiple reasoning paths ‚Üí consistent final answer.  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f8a4a7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from groq import Groq\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "client = Groq(api_key=api_key)\n",
    "\n",
    "# Deliberate Prompt Example: Multiple Reasoning Paths\n",
    "deliberate_prompt = \"\"\"\n",
    "Multiply 23 by 47. \n",
    "Think of at least two different ways to solve this, \n",
    "then provide the final consistent answer.\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",   # valid Groq model\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": deliberate_prompt}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a7d7567e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Method 1: Multiplication by Multiplication**\n",
      "\n",
      "23 √ó 40 = 920\n",
      "23 √ó 7 = 161\n",
      "Adding these two results together, we get:\n",
      "920 + 161 = 1081\n",
      "\n",
      "**Method 2: The Commutative Property and Multiplication by Multiplication**\n",
      "\n",
      "47 √ó 20 = 940\n",
      "47 √ó 3 = 141\n",
      "Adding these two results together, we get:\n",
      "940 + 141 = 1081\n",
      "\n",
      "**Verification: Multiplication with a calculator**\n",
      "\n",
      "23 √ó 47 = 1081\n",
      "\n",
      "**Final Answer:** \n",
      "The consistent answer to 23 √ó 47 is 1081.\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d5544d",
   "metadata": {},
   "source": [
    "## 7. Prompt Chaining in Generative AI\n",
    "\n",
    "---\n",
    "\n",
    "###  Definition\n",
    "Prompt Chaining is a technique where **multiple prompts are linked together in sequence**, with the output of one prompt serving as the input for the next.  \n",
    "- Instead of asking the model to solve a complex task in one go, you **break it down into smaller steps**.  \n",
    "- Each step is handled by a separate prompt, creating a logical chain that leads to the final result.\n",
    "\n",
    "---\n",
    "\n",
    "### How It Works\n",
    "1. **Break Down the Task** ‚Üí Divide a complex problem into smaller, manageable subtasks.  \n",
    "2. **Design Sequential Prompts** ‚Üí Each prompt solves one subtask.  \n",
    "3. **Chain Outputs** ‚Üí The output of one prompt becomes the input for the next.  \n",
    "4. **Final Result** ‚Üí The last prompt produces the complete solution.  \n",
    "\n",
    "---\n",
    "\n",
    "### Why It Matters\n",
    "- **Improves Accuracy** ‚Üí Reduces errors by tackling tasks step by step.  \n",
    "- **Scalability** ‚Üí Useful for multi-stage workflows (e.g., research, summarization, analysis).  \n",
    "- **Transparency** ‚Üí Easier to debug and understand where mistakes occur.  \n",
    "- **Flexibility** ‚Üí Each step can be customized or replaced independently.  \n",
    "\n",
    "---\n",
    "\n",
    "### Use Cases\n",
    "- **Research Workflows** ‚Üí First extract key points, then summarize, then generate insights.  \n",
    "- **Data Processing** ‚Üí Clean data ‚Üí classify ‚Üí format into JSON.  \n",
    "- **Creative Writing** ‚Üí Generate outline ‚Üí expand into draft ‚Üí polish into final text.  \n",
    "- **Education** ‚Üí Step-by-step problem solving (e.g., math or programming exercises).  \n",
    "\n",
    "---\n",
    "\n",
    "## Example (Conceptual)\n",
    "**Step 1 Prompt:**  \n",
    "\"Summarize the importance of AI in 3 bullet points.\"  \n",
    "\n",
    "**Step 1 Output:**  \n",
    "- AI automates tasks.  \n",
    "- AI improves decision-making.  \n",
    "- AI drives innovation.  \n",
    "\n",
    "**Step 2 Prompt (using Step 1 output):**  \n",
    "\"Expand each bullet point into a short paragraph with examples.\"  \n",
    "\n",
    "**Step 2 Output:**  \n",
    "- AI automates tasks such as customer support chatbots and manufacturing processes...  \n",
    "- AI improves decision-making in healthcare diagnostics and financial forecasting...  \n",
    "- AI drives innovation in self-driving cars, personalized education, and drug discovery...  \n",
    "\n",
    "---\n",
    "\n",
    "## Key Insights\n",
    "- Prompt Chaining is about **breaking complex tasks into smaller linked prompts**.  \n",
    "- Compared to other techniques:  \n",
    "  - **Instruction Prompt** ‚Üí Direct command.  \n",
    "  - **Zero‚ÄëShot Prompt** ‚Üí No examples, relies on pretrained knowledge.  \n",
    "  - **Few‚ÄëShot Prompt** ‚Üí A few examples guide the output.  \n",
    "  - **Chain‚Äëof‚ÄëThought Prompt** ‚Üí Explicit reasoning steps before the answer.  \n",
    "  - **Role Prompting** ‚Üí Assigns a persona to shape tone and style.  \n",
    "  - **Deliberate Prompting** ‚Üí Multiple reasoning paths ‚Üí consistent final answer.  \n",
    "  - **Prompt Chaining** ‚Üí Sequential prompts ‚Üí step-by-step workflow.  \n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d6163ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from groq import Groq\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "client = Groq(api_key=api_key)\n",
    "\n",
    "# Step 1: Summarization\n",
    "step1_prompt = \"Summarize the importance of AI in 3 bullet points.\"\n",
    "\n",
    "step1_response = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": step1_prompt}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9eb04439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 Output:\n",
      " Here's a summary of the importance of AI in three bullet points:\n",
      "\n",
      "‚Ä¢ **Enhanced Efficiency and Productivity**: Artificial intelligence (AI) can automate routine tasks, freeing up human resources for more strategic and creative work. AI-powered tools can process large amounts of data quickly and accurately, enabling businesses to make data-driven decisions and improve overall efficiency.\n",
      "\n",
      "‚Ä¢ **Improved Accuracy and Decision-Making**: AI algorithms can analyze complex patterns and make predictions or suggestions based on that analysis. This leads to more accurate decision-making, reduced risk, and better outcomes in various fields such as healthcare, finance, and transportation.\n",
      "\n",
      "‚Ä¢ **Personalization and Innovation**: AI enables businesses to provide personalized experiences to customers through tailored recommendations, content creation, and customized products. Additionally, AI fosters innovation by allowing researchers to simulate complex scenarios, predict outcomes, and create new technologies that can solve real-world problems.\n"
     ]
    }
   ],
   "source": [
    "step1_output = step1_response.choices[0].message.content\n",
    "print(\"Step 1 Output:\\n\", step1_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ca4cab37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Expansion using Step 1 output\n",
    "step2_prompt = f\"Expand each of the following points into a short paragraph with examples:\\n{step1_output}\"\n",
    "\n",
    "step2_response = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": step2_prompt}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e8a4ac33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 2 Output:\n",
      " **Enhanced Efficiency and Productivity**\n",
      "\n",
      "The integration of artificial intelligence (AI) into various industries has greatly improved efficiency and productivity. For instance, AI-powered tools can automate routine tasks such as data entry, scheduling, and email management, freeing up human resources to focus on more strategic and creative work. A notable example is the use of AI in customer service, where chatbots can handle basic queries and provide prompt responses, reducing the workload of customer support teams. This enables businesses to allocate more time and resources to developing new products and services, and ultimately, improve their competitive edge. Additionally, AI can also optimize processes such as supply chain management and resource allocation, leading to significant cost savings and improved productivity.\n",
      "\n",
      "**Improved Accuracy and Decision-Making**\n",
      "\n",
      "Artificial intelligence algorithms are capable of analyzing vast amounts of complex data, making predictions, and providing valuable insights that lead to more accurate decision-making. In the medical field, AI-powered diagnostic tools can analyze medical images and patient data to identify potential health risks, enabling doctors to provide more accurate diagnoses and treatment plans. For example, AI-based systems can help identify cancerous tumors in medical images with a high degree of accuracy, allowing for timely treatment and improved patient outcomes. Similarly, in finance, AI algorithms can analyze market trends, customer behavior, and financial data to predict market fluctuations and make informed investment decisions. This leads to reduced risk and better financial performance for businesses and individuals.\n",
      "\n",
      "**Personalization and Innovation**\n",
      "\n",
      "Artificial intelligence enables businesses to provide personalized experiences to customers through tailored recommendations, content creation, and customized products. For instance, online retailers like Amazon use AI-powered recommendation engines to suggest products based on a customer's browsing and purchasing history. This leads to increased customer satisfaction and loyalty, as customers feel that the retailer understands their preferences and needs. Moreover, AI can foster innovation by allowing researchers to simulate complex scenarios, predict outcomes, and create new technologies that can solve real-world problems. For example, AI-powered simulation tools can help engineers design and test new aircraft models, reducing the need for physical prototypes and speeding up the development process. This enables businesses to stay ahead of the competition and develop innovative solutions that meet the evolving needs of their customers.\n"
     ]
    }
   ],
   "source": [
    "step2_output = step2_response.choices[0].message.content\n",
    "print(\"\\nStep 2 Output:\\n\", step2_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6df3c8",
   "metadata": {},
   "source": [
    "### Prompt Chaining ‚Äî What Happens Here\n",
    "\n",
    "---\n",
    "- **Step 1 Prompt** ‚Üí The model summarizes AI in 3 bullet points.  \n",
    "- **Step 1 Output** ‚Üí Concise summary (e.g., automation, decision-making, innovation).  \n",
    "- **Step 2 Prompt** ‚Üí Uses Step 1 output as input, asking the model to expand each point.  \n",
    "- **Step 2 Output** ‚Üí Detailed explanation with examples.  \n",
    "\n",
    "---\n",
    "\n",
    "#### Why Prompt Chaining is Useful\n",
    "- Breaks down **complex tasks into smaller steps**.  \n",
    "- Ensures **clarity and accuracy** at each stage.  \n",
    "- Makes workflows **modular and reusable** (students can swap Step 2 for formatting, JSON conversion, etc.).  \n",
    "\n",
    "---\n",
    "\n",
    "#### Insight\n",
    "Prompt Chaining is especially powerful for **multi-stage workflows** like research, education, and creative writing.  \n",
    "It allows you to **control the process step by step**, ensuring that each stage produces reliable and structured outputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb21512",
   "metadata": {},
   "source": [
    "## 8. ReAct Prompting (Reason + Act)\n",
    "\n",
    "---\n",
    "\n",
    "### Definition\n",
    "ReAct Prompting is a technique where the **Large Language Model (LLM)** is guided to **combine reasoning with actions**.  \n",
    "- The model first **reasons step by step** about the problem.  \n",
    "- Then it **takes an action** (e.g., calling a tool, retrieving data, or producing an answer).  \n",
    "- This loop continues until the task is solved.\n",
    "\n",
    "---\n",
    "\n",
    "### How It Works\n",
    "1. **Reasoning Phase** ‚Üí The model explains its thought process.  \n",
    "2. **Action Phase** ‚Üí The model executes an action (e.g., search, calculation, API call).  \n",
    "3. **Observation Phase** ‚Üí The model receives results from the action.  \n",
    "4. **Repeat if Needed** ‚Üí The model reasons again and takes further actions until the final answer is ready.  \n",
    "\n",
    "---\n",
    "\n",
    "### Why It Matters\n",
    "- **Bridges Thinking and Doing** ‚Üí The model doesn‚Äôt just ‚Äúthink,‚Äù it also ‚Äúacts.‚Äù  \n",
    "- **Dynamic Problem Solving** ‚Üí Useful for tasks requiring external data or iterative steps.  \n",
    "- **Transparency** ‚Üí Shows reasoning and actions clearly.  \n",
    "- **Agentic AI** ‚Üí Forms the foundation of autonomous agents that interact with tools.  \n",
    "\n",
    "---\n",
    "\n",
    "### Use Cases\n",
    "- **Information Retrieval** ‚Üí Reason about what‚Äôs needed, then call a search API.  \n",
    "- **Math & Logic** ‚Üí Reason step by step, then calculate using a tool.  \n",
    "- **Task Automation** ‚Üí Break down workflow, act on each step, and combine results.  \n",
    "- **Interactive Agents** ‚Üí Chatbots that reason and fetch live data.  \n",
    "\n",
    "---\n",
    "\n",
    "### Example (Conceptual)\n",
    "**Prompt:**  \n",
    "\"Find the current population of Japan and explain if it has grown compared to 2020.\"\n",
    "\n",
    "**Model Response (ReAct style):**  \n",
    "- **Reasoning:** To answer, I need Japan‚Äôs current population and compare it with 2020 data.  \n",
    "- **Action:** Call a search tool for \"current population of Japan.\"  \n",
    "- **Observation:** Population ~124 million (2026).  \n",
    "- **Reasoning:** In 2020, Japan‚Äôs population was ~126 million.  \n",
    "- **Final Answer:** Japan‚Äôs population has declined slightly since 2020.  \n",
    "\n",
    "---\n",
    "\n",
    "### Key Insights\n",
    "- ReAct Prompting combines **Chain-of-Thought reasoning** with **tool usage/actions**.  \n",
    "- Compared to other techniques:  \n",
    "  - **Instruction Prompt** ‚Üí Direct command.  \n",
    "  - **Zero‚ÄëShot Prompt** ‚Üí No examples, relies on pretrained knowledge.  \n",
    "  - **Few‚ÄëShot Prompt** ‚Üí A few examples guide the output.  \n",
    "  - **Chain‚Äëof‚ÄëThought Prompt** ‚Üí Explicit reasoning steps before the answer.  \n",
    "  - **Role Prompting** ‚Üí Assigns a persona to shape tone and style.  \n",
    "  - **Deliberate Prompting** ‚Üí Multiple reasoning paths ‚Üí consistent final answer.  \n",
    "  - **Prompt Chaining** ‚Üí Sequential prompts ‚Üí step-by-step workflow.  \n",
    "  - **ReAct Prompting** ‚Üí Combines reasoning + tool actions for dynamic problem solving.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "32d21520",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from groq import Groq\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "client = Groq(api_key=api_key)\n",
    "\n",
    "# ReAct Prompt Example: Reason + Act\n",
    "react_prompt = \"\"\"\n",
    "You are an assistant that uses Reason + Act.\n",
    "Question: Multiply 23 by 47.\n",
    "First, reason step by step about how to solve it.\n",
    "Then, act by giving the final answer.\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",   # valid Groq model\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": react_prompt}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e010073c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Reason:**\n",
      "\n",
      "To multiply two numbers, we need to follow the steps of the multiplication algorithm. \n",
      "\n",
      "First, we multiply each digit of the second number (47) by the first number (23). Here's how we do it: \n",
      "\n",
      "1. Multiply 23 by 40 (the tens digit of 47): \n",
      "  - 23 √ó 40 = 920 (since 40 is 4 tens, it is the same as 23 √ó 4 √ó 10)\n",
      "  - Keep track of the tens place\n",
      "\n",
      "2. Multiply 23 by 7 (the ones digit of 47):\n",
      "  - 23 √ó 7 = 161 \n",
      "\n",
      "Now we need to add the numbers we got in steps 1 and 2, while considering their places:\n",
      "- We add 920 (from the tens place) and 0 (because 161 is less than 23 √ó 10, thus not carrying) to it, which will result in 920.\n",
      "\n",
      "- Now add 100 (the tens digit of 23 √ó 40 shifted by one place) and 61 (the ones digit of the 161 multiplied by 7)\n",
      "This yields 1000 + 980 + 161 which is the same as (980 + 161) + (1000). \n",
      "So  the result is 1141  or 1381.\n",
      "\n",
      "So the multiplication problem 23 √ó 47 is the same as the multiplication of the tens digits  (2 √ó 4) plus tens place shifted  2 √ó 7 or (2 √ó 7) (2√ó 0), plus (2 x7) plus the ones place shifted of  2 √ó 4.\n",
      "\n",
      "The result we're looking for should be found by shifting the last calculation of 2 √ó 7, one position so 2 √ó 7 shifted should actually be 2 √ó 70  as we're shifting the numbers 2 √ó 4 and  2 √ó 7.\n",
      "\n",
      "2 √ó 70 + 2 √ó 7 = 140 + 14 \n",
      "Since this is not the correct operation that we need to solve the equation, we instead multiply (2 √ó 4) (23) by the number 10 since 4 is 4 tens. We get 2√ó 4  √ó 10 which is  80 + (2 √ó 4  √ó 10) and (23) with the number 7 (which results in, 161). The equation (2 √ó  7) with the shifted 100  from previous equation becomes (80+161)+((20+2)*7=1000+920+ 21) or the original numbers  and just shift the multiplication 2 √ó 7 by 1 position in order for the 2√ó10 and the 7 in 23 √ó 70 to equal out with multiplication (2  √ó  7) 23  shifted which actually equals 1610 + (20 + (40)) 7  or more properly (23  √ó 7 shifted) plus 10 and 20 ( 20 + 2  * 7). We can instead take ( (2√ó 4) with the tens shifted to get (2 √ó  4) √ó  10) as (2 √ó 4) with a shifted tens place of ten  or (2 √ó 40) or 80. Now with the equation (2 √ó  40)  or 80 + 161, which is shifted to 920 and the original equation of (2 √ó 7) with  a shifted 100 or 23 √ó 40. \n",
      "\n",
      "Since we had 920 + (20 + 2)* 7 (or 920 +  21) we should multiply the tens place by 10. In this case the 2  * 40. Then we multiply by (2  * 7 shifted) or the original (23 * 70)  and add it to the number 23 √ó 40 (which is 920)  which results in (920 + 2 √ó 7  √ó 10)(2 √ó 40 with the shifted 10) which will give us 920 +  2√ó70 (which is 140 + 20). In this step the calculation of the numbers in the equation ( 920 + (20 + 2) *7) or ( 920 +  21) is actually  2 √ó 70 shifted (or the  2 √ó 70  as 140 and 20) in order for this calculation to be correct: The  920 shifted  10 or (20) + (2 * 7 shifted by 10) in order for the numbers in equation 920 + (2*70) shifted by  21.\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27484ffe",
   "metadata": {},
   "source": [
    "##  ReAct Prompting ‚Äî What Happens Here\n",
    "\n",
    "---\n",
    "\n",
    "###  What Happens Here\n",
    "- **Reasoning Phase** ‚Üí The model explains the steps (e.g., break 47 into 40 + 7, compute separately).  \n",
    "- **Action Phase** ‚Üí The model provides the final answer (1081).  \n",
    "- This demonstrates how **ReAct combines thinking + doing** in one prompt.  \n",
    "\n",
    "---\n",
    "\n",
    "###  Why ReAct Prompting is Useful\n",
    "- **Dynamic workflows** ‚Üí Useful when tasks require both reasoning and tool actions.  \n",
    "- **Transparency** ‚Üí Shows the thought process before the final answer.  \n",
    "- **Agentic AI foundation** ‚Üí Enables models to act like agents that reason and interact with tools.  \n",
    "\n",
    "---\n",
    "\n",
    "###  Insight\n",
    "ReAct Prompting is especially powerful for **agentic AI systems** where the model needs to both **reason logically** and **perform actions** (like searching, calculating, or retrieving data).  \n",
    "It bridges the gap between **thought** and **execution**, making AI more interactive and reliable in real-world workflows.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8332d7c6",
   "metadata": {},
   "source": [
    "## 9. Contextual Prompting in Generative AI\n",
    "\n",
    "---\n",
    "\n",
    "####  Definition\n",
    "Contextual Prompting is a technique where you **provide background information, context, or supporting details** before asking the model to perform a task.  \n",
    "- Instead of a bare question, you **frame the query with relevant context**.  \n",
    "- This helps the model generate responses that are more accurate, relevant, and aligned with the situation.\n",
    "\n",
    "---\n",
    "\n",
    "####  How It Works\n",
    "1. **Provide Background** ‚Üí Add details about the scenario, domain, or constraints.  \n",
    "2. **Ask the Question** ‚Üí Follow up with the actual task or query.  \n",
    "3. **Model Response** ‚Üí The model uses the provided context to tailor its answer.  \n",
    "\n",
    "---\n",
    "\n",
    "####  Why It Matters\n",
    "- **Improves Relevance** ‚Üí The model adapts its answer to the given situation.  \n",
    "- **Domain Adaptation** ‚Üí Useful when working in specialized fields (medicine, law, education).  \n",
    "- **Personalization** ‚Üí Makes responses more tailored to the user‚Äôs needs.  \n",
    "- **Reduces Ambiguity** ‚Üí Context clarifies vague or open-ended questions.  \n",
    "\n",
    "---\n",
    "\n",
    "####  Use Cases\n",
    "- **Education** ‚Üí \"You are teaching 10-year-olds about fractions. Explain in simple terms.\"  \n",
    "- **Business** ‚Üí \"We are preparing a pitch for a healthcare startup. Suggest 3 key talking points.\"  \n",
    "- **Programming** ‚Üí \"In the context of Python data science, explain how NumPy arrays differ from lists.\"  \n",
    "- **Creative Writing** ‚Üí \"Imagine a futuristic city in 2100. Write a short description of daily life.\"  \n",
    "\n",
    "---\n",
    "\n",
    "####  Example (Conceptual)\n",
    "**Prompt:**  \n",
    "\"Context: You are helping a beginner learn Python.  \n",
    "Task: Explain how loops work with a simple example.\"\n",
    "\n",
    "**Model Response (Contextual):**  \n",
    "\"Since the learner is a beginner, I‚Äôll explain loops in simple terms.  \n",
    "A loop lets you repeat actions. For example:  \n",
    "```python\n",
    "for i in range(5):\n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0e91f4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from groq import Groq\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "client = Groq(api_key=api_key)\n",
    "\n",
    "# Contextual Prompt Example: Teaching Loops to a Beginner\n",
    "contextual_prompt = \"\"\"\n",
    "Context: You are helping a beginner learn Python.\n",
    "Task: Explain how loops work with a simple example.\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",   # valid Groq model\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": contextual_prompt}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4d7cb86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loops are a fundamental concept in programming that allow you to repeat a block of code multiple times. In this example, we'll use a simple print loop to demonstrate how loops work.\n",
      "\n",
      "### Basic Syntax\n",
      "\n",
      "In Python, there are two main types of loops:\n",
      "\n",
      "1. **For Loop**: This loop is used when you know the number of iterations in advance.\n",
      "2. **While Loop**: This loop is used when you don't know the number of iterations in advance.\n",
      "\n",
      "### For Loop\n",
      "\n",
      "A for loop is used to iterate over a sequence (like a list, tuple, dictionary, etc.) or other iterable objects.\n",
      "\n",
      "```python\n",
      "# Example: Print the numbers from 1 to 5\n",
      "fruits = ['apple', 'banana', 'cherry']\n",
      "for fruit in fruits:\n",
      "    print(fruit)\n",
      "```\n",
      "\n",
      "In this example, we have a list of fruits. The `for` loop iterates over each item in the list and prints it.\n",
      "\n",
      "Here's how it works:\n",
      "\n",
      "1. The loop starts at the beginning of the list.\n",
      "2. It assigns the first item (`'apple'`) to the variable `fruit`.\n",
      "3. It executes the `print(fruit)` statement.\n",
      "4. It moves on to the next item in the list (`'banana'`) and repeats the process.\n",
      "5. It continues until it reaches the end of the list (`'cherry'`).\n",
      "\n",
      "### While Loop\n",
      "\n",
      "A while loop is used to repeat a block of code while a certain condition is met.\n",
      "\n",
      "```python\n",
      "# Example: Print numbers from 1 to 5\n",
      "i = 1\n",
      "while i <= 5:\n",
      "    print(i)\n",
      "    i += 1\n",
      "```\n",
      "\n",
      "In this example, we have a while loop that prints numbers from 1 to 5.\n",
      "\n",
      "Here's how it works:\n",
      "\n",
      "1. The loop starts with `i` set to 1.\n",
      "2. The condition `i <= 5` is evaluated. If it's true, the loop body is executed.\n",
      "3. The `print(i)` statement is executed with `i` set to 1.\n",
      "4. The `i += 1` statement increments `i` to 2.\n",
      "5. The loop body is repeated until the condition `i <= 5` is no longer true.\n",
      "\n",
      "### Best Practices\n",
      "\n",
      "* Use meaningful variable names.\n",
      "* Keep loops concise and focused on the task at hand.\n",
      "* Avoid excessive nesting of loops.\n",
      "\n",
      "I hope this example helps you understand how loops work in Python!\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9419add5",
   "metadata": {},
   "source": [
    "#### Contextual Prompting ‚Äî What Happens Here\n",
    "\n",
    "---\n",
    "- **Context Provided** ‚Üí The model is told the learner is a beginner.  \n",
    "- **Task Asked** ‚Üí Explain loops in Python.  \n",
    "- **Response Style** ‚Üí The model adapts its explanation to be simple, beginner‚Äëfriendly, and uses easy examples.  \n",
    "\n",
    "---\n",
    "\n",
    "####  Why Contextual Prompting is Useful\n",
    "- Ensures the answer is **tailored to the situation** (beginner vs expert).  \n",
    "- Reduces ambiguity by **framing the query with background info**.  \n",
    "- Makes responses **more relevant, accurate, and personalized**.  \n",
    "\n",
    "---\n",
    "\n",
    "#### Insight\n",
    "Contextual Prompting is especially powerful when you want the model to **adapt its response to a specific audience, domain, or scenario**.  \n",
    "By embedding background information, you guide the model to produce **clearer, more context‚Äëaware outputs**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44f5159",
   "metadata": {},
   "source": [
    "## 10. Conversational Prompting in Generative AI\n",
    "\n",
    "---\n",
    "\n",
    "####  Definition\n",
    "Conversational Prompting is a technique where the model is guided to **simulate dialogue with alternating turns**.  \n",
    "- Instead of a single command, the prompt is structured as a back‚Äëand‚Äëforth conversation.  \n",
    "- This helps the model generate responses that feel natural, interactive, and context‚Äëaware.  \n",
    "\n",
    "---\n",
    "\n",
    "####  How It Works\n",
    "1. **Set the Dialogue Context** ‚Üí Provide a scenario or role (e.g., teacher and student).  \n",
    "2. **Alternate Turns** ‚Üí Write prompts as if two people are talking.  \n",
    "3. **Model Response** ‚Üí The model continues the dialogue in a conversational style.  \n",
    "\n",
    "---\n",
    "\n",
    "####  Why It Matters\n",
    "- **Natural Flow** ‚Üí Produces outputs that mimic human conversation.  \n",
    "- **Engagement** ‚Üí Useful for training, simulations, or role‚Äëplay scenarios.  \n",
    "- **Context Retention** ‚Üí The model remembers previous turns and builds on them.  \n",
    "- **Interactive Learning** ‚Üí Great for teaching or coaching environments.  \n",
    "\n",
    "---\n",
    "\n",
    "####  Use Cases\n",
    "- **Education** ‚Üí Simulate teacher‚Äëstudent Q&A sessions.  \n",
    "- **Customer Support** ‚Üí Practice support agent and customer dialogues.  \n",
    "- **Interview Prep** ‚Üí Mock interviews with alternating questions and answers.  \n",
    "- **Storytelling** ‚Üí Characters talking to each other in a narrative.  \n",
    "\n",
    "---\n",
    "\n",
    "####  Example (Conceptual)\n",
    "**Prompt:**  \n",
    "\"Simulate a conversation between a teacher and a student learning Python.  \n",
    "Teacher: What is a loop?  \n",
    "Student: I‚Äôm not sure. Could you explain?  \n",
    "Teacher: ‚Ä¶\"\n",
    "\n",
    "**Model Response (Conversational):**  \n",
    "\"Teacher: A loop is a way to repeat actions. For example, in Python:  \n",
    "```python\n",
    "for i in range(5):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dc2bf65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from groq import Groq\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "client = Groq(api_key=api_key)\n",
    "\n",
    "# Conversational Prompt Example: Teacher-Student Dialogue\n",
    "conversational_prompt = \"\"\"\n",
    "Simulate a conversation between a teacher and a student learning Python.\n",
    "\n",
    "Teacher: What is a loop?\n",
    "Student: I‚Äôm not sure. Could you explain?\n",
    "Teacher:\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",   # valid Groq model\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": conversational_prompt}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3a1cacaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher: A loop is a control structure in Python (and programming languages in general) that allows you to repeat a block of code multiple times. Think of it like a conveyor belt - you can place items on the belt and it will keep moving until it reaches the end.\n",
      "\n",
      "Imagine you're making copies of a document. Instead of copying it manually each time, you can use a loop to tell the copier to make copies until you're out of paper. This saves you a lot of time and effort.\n",
      "\n",
      "In Python, there are two main types of loops: for loops and while loops. A for loop is used when you know how many times you want to repeat the code, but a while loop is used when you're not sure how many times you want to repeat the code, and it depends on a certain condition being met.\n",
      "\n",
      "Would you like to see an example of a for loop in action?\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4316697f",
   "metadata": {},
   "source": [
    "#### Conversational Prompting ‚Äî What Happens Here\n",
    "\n",
    "---\n",
    "\n",
    "#### What Happens Here\n",
    "- **Prompt Structure** ‚Üí Written as a dialogue with alternating turns.  \n",
    "- **Model Response** ‚Üí Continues the conversation naturally, explaining loops in a teacher‚Äëstudent style.  \n",
    "- **Outcome** ‚Üí Produces interactive, context‚Äëaware dialogue that feels human‚Äëlike.  \n",
    "\n",
    "---\n",
    "\n",
    "#### Why Conversational Prompting is Useful\n",
    "- Creates natural, flowing dialogue for simulations or role‚Äëplay.  \n",
    "- Helps in education, customer support, and interview practice.  \n",
    "- Ensures the model retains context across turns and builds on prior exchanges.  \n",
    "\n",
    "---\n",
    "\n",
    "#### Insight\n",
    "Conversational Prompting is especially powerful when you want the model to **simulate dialogue**.  \n",
    "By structuring prompts as alternating turns, you guide the model to produce **interactive, context‚Äëaware outputs** that feel more human and engaging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c6f1a3",
   "metadata": {},
   "source": [
    "#### 11. Multimodal Prompting in Generative AI\n",
    "\n",
    "---\n",
    "\n",
    "####  Definition\n",
    "Multimodal Prompting is a technique where the model is guided using **multiple types of input** such as text, images, audio, or video.  \n",
    "- Instead of relying only on text, you combine different modalities to enrich the prompt.  \n",
    "- This allows the model to generate responses that integrate information across formats.  \n",
    "\n",
    "---\n",
    "\n",
    "#### How It Works\n",
    "1. **Provide Multiple Inputs** ‚Üí Text plus another modality (image, audio, video).  \n",
    "2. **Frame the Task** ‚Üí Ask the model to analyze or combine these inputs.  \n",
    "3. **Model Response** ‚Üí The model produces an output that integrates the modalities.  \n",
    "\n",
    "---\n",
    "\n",
    "#### Why It Matters\n",
    "- **Richer Understanding** ‚Üí Models can interpret visual, auditory, and textual cues together.  \n",
    "- **Enhanced Creativity** ‚Üí Useful for storytelling, design, and multimedia projects.  \n",
    "- **Practical Applications** ‚Üí Enables tasks like captioning, visual Q&A, and multimodal search.  \n",
    "- **Human‚Äëlike Interaction** ‚Üí Mirrors how people process information from multiple senses.  \n",
    "\n",
    "---\n",
    "\n",
    "#### Use Cases\n",
    "- **Education** ‚Üí Provide a diagram plus text, ask the model to explain.  \n",
    "- **Healthcare** ‚Üí Combine patient notes with medical images for analysis.  \n",
    "- **Business** ‚Üí Use charts plus text to generate insights.  \n",
    "- **Creative Writing** ‚Üí Provide an image and ask for a descriptive story.  \n",
    "\n",
    "---\n",
    "\n",
    "#### Example (Conceptual)\n",
    "**Prompt:**  \n",
    "\"Here is an image of a bar chart showing sales growth from 2020 to 2025.  \n",
    "Task: Write a short summary explaining the trend.\"\n",
    "\n",
    "**Model Response (Multimodal):**  \n",
    "\"The chart shows steady growth in sales from 2020 to 2025, with the largest increase between 2023 and 2024. This suggests strong market expansion during that period.\"  \n",
    "\n",
    "---\n",
    "\n",
    "#### Key Insights\n",
    "- Multimodal Prompting is about **combining text with other inputs**.  \n",
    "- Compared to other techniques:  \n",
    "  - **Instruction Prompt** ‚Üí Direct command.  \n",
    "  - **Zero‚ÄëShot Prompt** ‚Üí No examples, relies on pretrained knowledge.  \n",
    "  - **Few‚ÄëShot Prompt** ‚Üí A few examples guide the output.  \n",
    "  - **Chain‚Äëof‚ÄëThought Prompt** ‚Üí Explicit reasoning steps before the answer.  \n",
    "  - **Role Prompting** ‚Üí Assigns a persona to shape tone and style.  \n",
    "  - **Deliberate Prompting** ‚Üí Multiple reasoning paths ‚Üí consistent final answer.  \n",
    "  - **Prompt Chaining** ‚Üí Sequential prompts ‚Üí step‚Äëby‚Äëstep workflow.  \n",
    "  - **ReAct Prompting** ‚Üí Combines reasoning + tool actions.  \n",
    "  - **Contextual Prompting** ‚Üí Provides background info ‚Üí tailored response.  \n",
    "  - **Conversational Prompting** ‚Üí Simulates dialogue with alternating turns.  \n",
    "  - **Multimodal Prompting** ‚Üí Combines text + image/audio/video for richer interaction.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e6a5d118",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from groq import Groq\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "client = Groq(api_key=api_key)\n",
    "\n",
    "# Multimodal Prompt Example: Text + Image Description\n",
    "multimodal_prompt = \"\"\"\n",
    "You are given a description of an image:\n",
    "'The image shows a bar chart of sales growth from 2020 to 2025, \n",
    "with steady increases each year and the largest jump between 2023 and 2024.'\n",
    "\n",
    "Task: Write a short summary explaining the trend in sales growth.\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",   # valid Groq model\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": multimodal_prompt}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ce3a5271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sales growth trend from 2020 to 2025 exhibits a steady upward trajectory, with consistent increases each year. However, there is a notable surge in growth between 2023 and 2024, suggesting a significant acceleration in sales expansion during this period. Overall, the chart indicates a sustained and rapid growth in sales over the five-year period.\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c06e87f",
   "metadata": {},
   "source": [
    "#### Multimodal Prompting ‚Äî What Happens Here\n",
    "\n",
    "---\n",
    "\n",
    "#### What Happens Here\n",
    "- **Prompt Structure** ‚Üí Text plus a description of an image (simulated multimodal input).  \n",
    "- **Model Response** ‚Üí Summarizes the trend by integrating the described visual data.  \n",
    "- **Outcome** ‚Üí Shows how multimodal prompting can combine text with other modalities for richer insights.  \n",
    "\n",
    "---\n",
    "\n",
    "#### Why Multimodal Prompting is Useful\n",
    "- Enables the model to interpret and integrate multiple input types.  \n",
    "- Useful for education, business analytics, healthcare, and creative storytelling.  \n",
    "- Mirrors how humans process information from different senses together.  \n",
    "\n",
    "---\n",
    "\n",
    "#### Insight\n",
    "Multimodal Prompting is especially powerful when you want the model to **combine text with other modalities**.  \n",
    "By embedding descriptions of images, audio, or video alongside text, you guide the model to produce **richer, context‚Äëaware outputs** that go beyond text‚Äëonly reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13c9759",
   "metadata": {},
   "source": [
    "## 12. Retrieval‚ÄëAugmented Generation (RAG Prompting)\n",
    "\n",
    "---\n",
    "\n",
    "####  Definition\n",
    "RAG Prompting is a technique where the **Large Language Model (LLM)** is combined with an **external knowledge source** (such as a database, document store, or search engine).  \n",
    "- The model retrieves relevant information first.  \n",
    "- Then it generates a response using both the retrieved data and its own reasoning.  \n",
    "- This ensures answers are **accurate, up‚Äëto‚Äëdate, and grounded in external facts**.  \n",
    "\n",
    "---\n",
    "\n",
    "####  How It Works\n",
    "1. **User Query** ‚Üí The user asks a question.  \n",
    "2. **Retrieval Step** ‚Üí The system searches external sources (documents, APIs, web).  \n",
    "3. **Augmentation Step** ‚Üí Retrieved information is fed into the prompt.  \n",
    "4. **Generation Step** ‚Üí The LLM produces a final answer using both context + knowledge.  \n",
    "\n",
    "---\n",
    "\n",
    "####  Why It Matters\n",
    "- **Accuracy** ‚Üí Reduces hallucinations by grounding answers in real data.  \n",
    "- **Freshness** ‚Üí Provides up‚Äëto‚Äëdate information beyond the model‚Äôs training cutoff.  \n",
    "- **Domain Adaptation** ‚Üí Useful for specialized fields (medicine, law, finance).  \n",
    "- **Trustworthiness** ‚Üí Responses can be traced back to sources.  \n",
    "\n",
    "---\n",
    "\n",
    "####  Use Cases\n",
    "- **Customer Support** ‚Üí Retrieve FAQs or manuals, then generate tailored answers.  \n",
    "- **Education** ‚Üí Pull from textbooks or lecture notes to answer student queries.  \n",
    "- **Business Intelligence** ‚Üí Retrieve company data, then generate insights.  \n",
    "- **Healthcare** ‚Üí Retrieve medical guidelines, then explain treatment options.  \n",
    "\n",
    "---\n",
    "\n",
    "####  Example (Conceptual)\n",
    "**Prompt:**  \n",
    "\"Using the latest WHO data, summarize the global COVID‚Äë19 vaccination rate.\"\n",
    "\n",
    "**Model Response (RAG style):**  \n",
    "- **Retrieval:** WHO database shows vaccination coverage at ~70% globally (2025).  \n",
    "- **Generation:** \"As of 2025, about 70% of the global population has received at least one dose of a COVID‚Äë19 vaccine, with higher coverage in developed regions compared to developing ones.\"  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5430c04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from groq import Groq\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "client = Groq(api_key=api_key)\n",
    "\n",
    "# RAG Prompt Example: Retrieval + Generation\n",
    "rag_prompt = \"\"\"\n",
    "You are an assistant that uses Retrieval-Augmented Generation (RAG).\n",
    "Here is retrieved information from WHO (simulated):\n",
    "'As of 2025, about 70% of the global population has received at least one dose of a COVID-19 vaccine.'\n",
    "\n",
    "Task: Using this retrieved data, generate a short summary of the global COVID-19 vaccination rate.\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",   # valid Groq model\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": rag_prompt}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "793c1545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the retrieved information from WHO, I can generate a short summary:\n",
      "\n",
      "\"As of 2025, significant progress has been made in global COVID-19 vaccination efforts. Approximately 70% of the world's population has received at least one dose of a COVID-19 vaccine, indicating a substantial decline in reported cases and a reduced threat of severe illness.\"\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a4ec15",
   "metadata": {},
   "source": [
    "#### Retrieval‚ÄëAugmented Generation (RAG Prompting) ‚Äî What Happens Here\n",
    "\n",
    "---\n",
    "\n",
    "#### What Happens Here\n",
    "- **Prompt Structure** ‚Üí Includes both the user query and retrieved external knowledge (e.g., WHO data snippet).  \n",
    "- **Model Response** ‚Üí Uses the retrieved information to generate a grounded, accurate answer.  \n",
    "- **Outcome** ‚Üí Produces reliable, up‚Äëto‚Äëdate responses that can be traced back to external sources.  \n",
    "\n",
    "---\n",
    "\n",
    "#### Why RAG Prompting is Useful\n",
    "- Reduces hallucinations by grounding answers in **real data**.  \n",
    "- Provides **fresh, domain‚Äëspecific knowledge** beyond the model‚Äôs training cutoff.  \n",
    "- Ensures **trustworthiness** by linking responses to external sources.  \n",
    "- Enables **specialized applications** in education, healthcare, business, and customer support.  \n",
    "\n",
    "---\n",
    "\n",
    "#### Insight\n",
    "RAG Prompting is especially powerful when you need the model to **combine its reasoning with external knowledge**.  \n",
    "By retrieving relevant information first and then generating an answer, you guide the model to produce **accurate, context‚Äëaware outputs** that are both reliable and explainable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b877c232",
   "metadata": {},
   "source": [
    "#### Prompt Engineering Techniques ‚Äî Complete Cheat‚ÄëSheet (1‚Äì12)\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. Instruction Prompting\n",
    "**Definition:** Directly tell the model what to do.  \n",
    "**Example:** \"Summarize this article in 3 bullet points.\"  \n",
    "**Insight:** Best for straightforward tasks where clarity is key.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Zero‚ÄëShot Prompting\n",
    "**Definition:** Ask the model to perform a task without giving examples.  \n",
    "**Example:** \"Translate 'Hello, how are you?' into French.\"  \n",
    "**Insight:** Relies entirely on the model‚Äôs pretraining knowledge.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Few‚ÄëShot Prompting\n",
    "**Definition:** Provide a few examples before asking the model to perform a similar task.  \n",
    "**Example:**  \n",
    "\"Translate the following sentences into French:  \n",
    "- I am happy ‚Üí Je suis heureux  \n",
    "- She is a teacher ‚Üí Elle est professeur  \n",
    "Now translate: He is a doctor.\"  \n",
    "**Insight:** Helps guide the model toward the desired format or style.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Chain‚Äëof‚ÄëThought Prompting\n",
    "**Definition:** Encourage the model to reason step by step before giving the final answer.  \n",
    "**Example:** \"Explain your reasoning step by step before solving 23 √ó 47.\"  \n",
    "**Insight:** Improves accuracy in math, logic, and complex reasoning tasks.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. Role‚ÄëBased Prompting\n",
    "**Definition:** Assign the model a persona or role to shape its response style.  \n",
    "**Example:** \"You are a Python tutor. Explain loops to a beginner.\"  \n",
    "**Insight:** Useful for tailoring tone, depth, and perspective.\n",
    "\n",
    "---\n",
    "\n",
    "#### 6. Deliberate (Self‚ÄëConsistency) Prompting\n",
    "**Definition:** Ask the model to generate multiple reasoning paths and then choose the most consistent answer.  \n",
    "**Example:** \"Solve this math problem three different ways, then give the final consistent answer.\"  \n",
    "**Insight:** Reduces errors by cross‚Äëchecking reasoning.\n",
    "\n",
    "---\n",
    "\n",
    "#### 7. Prompt Chaining\n",
    "**Definition:** Break a task into multiple sequential prompts, each building on the previous.  \n",
    "**Example:**  \n",
    "Step 1: \"Summarize this article.\"  \n",
    "Step 2: \"From the summary, extract 3 key insights.\"  \n",
    "Step 3: \"Turn those insights into a LinkedIn post.\"  \n",
    "**Insight:** Useful for complex workflows and multi‚Äëstage tasks.\n",
    "\n",
    "---\n",
    "\n",
    "#### 8. ReAct Prompting (Reason + Act)\n",
    "**Definition:** Combine reasoning with actions (e.g., tool use, search, calculation).  \n",
    "**Example:**  \n",
    "- Reason: \"To answer, I need Japan‚Äôs current population.\"  \n",
    "- Act: \"Search for Japan‚Äôs population.\"  \n",
    "- Observation: \"Population ~124 million.\"  \n",
    "- Final Answer: \"Japan‚Äôs population has declined since 2020.\"  \n",
    "**Insight:** Foundation for agentic AI systems that both think and act.\n",
    "\n",
    "---\n",
    "\n",
    "#### 9. Contextual Prompting\n",
    "**Definition:** Provide background information before asking the question.  \n",
    "**Example:**  \n",
    "\"Context: You are helping a beginner learn Python.  \n",
    "Task: Explain how loops work with a simple example.\"  \n",
    "**Insight:** Makes responses more tailored, relevant, and audience‚Äëspecific.\n",
    "\n",
    "---\n",
    "\n",
    "#### 10. Conversational Prompting\n",
    "**Definition:** Structure prompts as dialogue with alternating turns.  \n",
    "**Example:**  \n",
    "Teacher: \"What is a loop?\"  \n",
    "Student: \"I‚Äôm not sure. Could you explain?\"  \n",
    "Teacher: ‚Ä¶  \n",
    "**Insight:** Produces natural, interactive, context‚Äëaware dialogue.\n",
    "\n",
    "---\n",
    "\n",
    "#### 11. Multimodal Prompting\n",
    "**Definition:** Combine text with other modalities (image, audio, video).  \n",
    "**Example:**  \n",
    "\"Here is an image of a bar chart showing sales growth.  \n",
    "Task: Write a short summary explaining the trend.\"  \n",
    "**Insight:** Enables richer, human‚Äëlike interaction by integrating multiple input types.\n",
    "\n",
    "---\n",
    "\n",
    "#### 12. Retrieval‚ÄëAugmented Generation (RAG Prompting)\n",
    "**Definition:** Retrieve external knowledge first, then generate an answer using it.  \n",
    "**Example:**  \n",
    "Retrieved Data: \"WHO reports 70% global vaccination coverage in 2025.\"  \n",
    "Prompt: \"Summarize the global COVID‚Äë19 vaccination rate using this data.\"  \n",
    "**Insight:** Ensures answers are accurate, up‚Äëto‚Äëdate, and grounded in real sources.\n",
    "\n",
    "---\n",
    "\n",
    "#### Final Insight\n",
    "Together, these **12 techniques** form a complete toolkit for prompt engineering:  \n",
    "- **Basic Control:** Instruction, Zero‚ÄëShot, Few‚ÄëShot  \n",
    "- **Reasoning:** Chain‚Äëof‚ÄëThought, Deliberate, Prompt Chaining  \n",
    "- **Agentic & Contextual:** ReAct, Contextual, Conversational  \n",
    "- **Advanced:** Multimodal, RAG  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b128426",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid #4CAF50\">\n",
    "\n",
    "#### 1. Instruction Prompting\n",
    "**Definition:** Directly tell the model what to do.  \n",
    "**Example:** \"Summarize this article in 3 bullet points.\"  \n",
    "**Insight:** Best for straightforward tasks where clarity is key.\n",
    "\n",
    "<hr style=\"border:2px solid #2196F3\">\n",
    "\n",
    "#### 2. Zero‚ÄëShot Prompting\n",
    "**Definition:** Ask the model to perform a task without giving examples.  \n",
    "**Example:** \"Translate 'Hello, how are you?' into French.\"  \n",
    "**Insight:** Relies entirely on the model‚Äôs pretraining knowledge.\n",
    "\n",
    "<hr style=\"border:2px solid #FF9800\">\n",
    "\n",
    "#### 3. Few‚ÄëShot Prompting\n",
    "**Definition:** Provide a few examples before asking the model to perform a similar task.  \n",
    "**Example:**  \n",
    "\"Translate the following sentences into French:  \n",
    "- I am happy ‚Üí Je suis heureux  \n",
    "- She is a teacher ‚Üí Elle est professeur  \n",
    "Now translate: He is a doctor.\"  \n",
    "**Insight:** Helps guide the model toward the desired format or style.\n",
    "\n",
    "<hr style=\"border:2px solid #9C27B0\">\n",
    "\n",
    "#### 4. Chain‚Äëof‚ÄëThought Prompting\n",
    "**Definition:** Encourage the model to reason step by step before giving the final answer.  \n",
    "**Example:** \"Explain your reasoning step by step before solving 23 √ó 47.\"  \n",
    "**Insight:** Improves accuracy in math, logic, and complex reasoning tasks.\n",
    "\n",
    "<hr style=\"border:2px solid #795548\">\n",
    "\n",
    "#### 5. Role‚ÄëBased Prompting\n",
    "**Definition:** Assign the model a persona or role to shape its response style.  \n",
    "**Example:** \"You are a Python tutor. Explain loops to a beginner.\"  \n",
    "**Insight:** Useful for tailoring tone, depth, and perspective.\n",
    "\n",
    "<hr style=\"border:2px solid #E91E63\">\n",
    "\n",
    "#### 6. Deliberate (Self‚ÄëConsistency) Prompting\n",
    "**Definition:** Ask the model to generate multiple reasoning paths and then choose the most consistent answer.  \n",
    "**Example:** \"Solve this math problem three different ways, then give the final consistent answer.\"  \n",
    "**Insight:** Reduces errors by cross‚Äëchecking reasoning.\n",
    "\n",
    "<hr style=\"border:2px solid #607D8B\">\n",
    "\n",
    "#### 7. Prompt Chaining\n",
    "**Definition:** Break a task into multiple sequential prompts, each building on the previous.  \n",
    "**Example:**  \n",
    "Step 1: \"Summarize this article.\"  \n",
    "Step 2: \"From the summary, extract 3 key insights.\"  \n",
    "Step 3: \"Turn those insights into a LinkedIn post.\"  \n",
    "**Insight:** Useful for complex workflows and multi‚Äëstage tasks.\n",
    "\n",
    "<hr style=\"border:2px solid #4CAF50\">\n",
    "\n",
    "#### 8. ReAct Prompting (Reason + Act)\n",
    "**Definition:** Combine reasoning with actions (e.g., tool use, search, calculation).  \n",
    "**Example:**  \n",
    "- Reason: \"To answer, I need Japan‚Äôs current population.\"  \n",
    "- Act: \"Search for Japan‚Äôs population.\"  \n",
    "- Observation: \"Population ~124 million.\"  \n",
    "- Final Answer: \"Japan‚Äôs population has declined since 2020.\"  \n",
    "**Insight:** Foundation for agentic AI systems that both think and act.\n",
    "\n",
    "<hr style=\"border:2px solid #2196F3\">\n",
    "\n",
    "#### 9. Contextual Prompting\n",
    "**Definition:** Provide background information before asking the question.  \n",
    "**Example:**  \n",
    "\"Context: You are helping a beginner learn Python.  \n",
    "Task: Explain how loops work with a simple example.\"  \n",
    "**Insight:** Makes responses more tailored, relevant, and audience‚Äëspecific.\n",
    "\n",
    "<hr style=\"border:2px solid #FF9800\">\n",
    "\n",
    "#### 10. Conversational Prompting\n",
    "**Definition:** Structure prompts as dialogue with alternating turns.  \n",
    "**Example:**  \n",
    "Teacher: \"What is a loop?\"  \n",
    "Student: \"I‚Äôm not sure. Could you explain?\"  \n",
    "Teacher: ‚Ä¶  \n",
    "**Insight:** Produces natural, interactive, context‚Äëaware dialogue.\n",
    "\n",
    "<hr style=\"border:2px solid #9C27B0\">\n",
    "\n",
    "#### 11. Multimodal Prompting\n",
    "**Definition:** Combine text with other modalities (image, audio, video).  \n",
    "**Example:**  \n",
    "\"Here is an image of a bar chart showing sales growth.  \n",
    "Task: Write a short summary explaining the trend.\"  \n",
    "**Insight:** Enables richer, human‚Äëlike interaction by integrating multiple input types.\n",
    "\n",
    "<hr style=\"border:2px solid #795548\">\n",
    "\n",
    "#### 12. Retrieval‚ÄëAugmented Generation (RAG Prompting)\n",
    "**Definition:** Retrieve external knowledge first, then generate an answer using it.  \n",
    "**Example:**  \n",
    "Retrieved Data: \"WHO reports 70% global vaccination coverage in 2025.\"  \n",
    "Prompt: \"Summarize the global COVID‚Äë19 vaccination rate using this data.\"  \n",
    "**Insight:** Ensures answers are accurate, up‚Äëto‚Äëdate, and grounded in real sources.\n",
    "\n",
    "<hr style=\"border:2px solid #000000\">\n",
    "\n",
    "#### Final Insight\n",
    "Together, these **12 techniques** form a complete toolkit for prompt engineering:  \n",
    "- **Basic Control:** Instruction, Zero‚ÄëShot, Few‚ÄëShot  \n",
    "- **Reasoning:** Chain‚Äëof‚ÄëThought, Deliberate, Prompt Chaining  \n",
    "- **Agentic & Contextual:** ReAct, Contextual, Conversational  \n",
    "- **Advanced:** Multimodal, RAG  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb8c9f7",
   "metadata": {},
   "source": [
    "#### Prompt Engineering Techniques ‚Äî Comparison Table\n",
    "\n",
    "| Technique | Definition | Example | Best Use Case |\n",
    "|-----------|------------|---------|---------------|\n",
    "| **Instruction Prompting** | Directly tell the model what to do. | \"Summarize this article in 3 bullet points.\" | Simple, clear tasks where precision is needed. |\n",
    "| **Zero‚ÄëShot Prompting** | Ask the model to perform a task without examples. | \"Translate 'Hello, how are you?' into French.\" | Quick tasks relying on pretrained knowledge. |\n",
    "| **Few‚ÄëShot Prompting** | Provide a few examples before the query. | \"Translate: I am happy ‚Üí Je suis heureux; She is a teacher ‚Üí Elle est professeur. Now translate: He is a doctor.\" | When you want the model to mimic a specific format or style. |\n",
    "| **Chain‚Äëof‚ÄëThought Prompting** | Encourage step‚Äëby‚Äëstep reasoning before the answer. | \"Explain your reasoning step by step before solving 23 √ó 47.\" | Math, logic, or complex reasoning problems. |\n",
    "| **Role‚ÄëBased Prompting** | Assign a persona or role to shape responses. | \"You are a Python tutor. Explain loops to a beginner.\" | Teaching, coaching, or tone‚Äëspecific tasks. |\n",
    "| **Deliberate (Self‚ÄëConsistency) Prompting** | Generate multiple reasoning paths, then choose the consistent answer. | \"Solve this math problem three different ways, then give the final consistent answer.\" | Reducing errors in reasoning‚Äëheavy tasks. |\n",
    "| **Prompt Chaining** | Break tasks into sequential prompts. | Step 1: Summarize article ‚Üí Step 2: Extract insights ‚Üí Step 3: Write LinkedIn post. | Multi‚Äëstage workflows, content pipelines. |\n",
    "| **ReAct Prompting (Reason + Act)** | Combine reasoning with tool actions. | Reason: Need Japan‚Äôs population ‚Üí Act: Search ‚Üí Observation: 124M ‚Üí Final Answer: Declined since 2020. | Agentic AI, tasks needing external tools or APIs. |\n",
    "| **Contextual Prompting** | Provide background info before asking. | \"Context: You are helping a beginner learn Python. Task: Explain loops simply.\" | Tailored, audience‚Äëspecific responses. |\n",
    "| **Conversational Prompting** | Structure prompts as dialogue with alternating turns. | Teacher: \"What is a loop?\" Student: \"I‚Äôm not sure.\" Teacher: ‚Ä¶ | Simulations, role‚Äëplay, interactive learning. |\n",
    "| **Multimodal Prompting** | Combine text with other inputs (image, audio, video). | \"Here is an image of a bar chart. Task: Summarize the trend.\" | Education, analytics, creative storytelling. |\n",
    "| **RAG Prompting** | Retrieve external knowledge, then generate an answer. | Retrieved Data: WHO reports 70% vaccination. Prompt: \"Summarize global vaccination rate.\" | Accurate, up‚Äëto‚Äëdate answers in specialized domains. |\n",
    "\n",
    "---\n",
    "\n",
    "#### Insight\n",
    "- **Basic Control:** Instruction, Zero‚ÄëShot, Few‚ÄëShot ‚Üí Best for simple tasks.  \n",
    "- **Reasoning:** Chain‚Äëof‚ÄëThought, Deliberate, Prompt Chaining ‚Üí Best for logic and multi‚Äëstep workflows.  \n",
    "- **Agentic & Contextual:** ReAct, Contextual, Conversational ‚Üí Best for interactive, tool‚Äëdriven, or audience‚Äëspecific tasks.  \n",
    "- **Advanced:** Multimodal, RAG ‚Üí Best for real‚Äëworld applications needing external data or multiple input types.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e8f3a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
