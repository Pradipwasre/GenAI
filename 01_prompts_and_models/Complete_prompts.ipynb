{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea46a5a5",
   "metadata": {},
   "source": [
    "# üß† All About Prompts\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Warm-Up & Context\n",
    "- Why prompts matter in GenAI\n",
    "- Small changes ‚Üí big differences in outputs\n",
    "- Prompt Engineering as an emerging skill/job role\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Vanilla Prompt Styles (Without LangChain)\n",
    "### Common types:\n",
    "- **Instruction Prompts** ‚Üí Direct commands (e.g., \"Summarize this article in 3 bullet points\")\n",
    "- **Zero-Shot Prompts** ‚Üí No examples, rely on model‚Äôs pretraining\n",
    "- **Few-Shot Prompts** ‚Üí Provide a few examples before the query\n",
    "- **Chain-of-Thought Prompts** ‚Üí Encourage reasoning step by step\n",
    "- **Role-Based Prompts** ‚Üí Assign persona (e.g., \"You are a Python tutor‚Ä¶\")\n",
    "- **Contextual Prompts** ‚Üí Provide background info before asking\n",
    "- **Conversational Prompts** ‚Üí Simulate dialogue with alternating turns\n",
    "- **Multimodal Prompts** ‚Üí Text + image/audio/video (if supported)\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Why LangChain?\n",
    "- **Challenge with vanilla prompts:**\n",
    "  - Hard to maintain consistency across applications\n",
    "  - No built-in validation ‚Üí errors if placeholders are missing\n",
    "  - Difficult to reuse prompts across projects\n",
    "  - Managing multi-turn conversations is messy\n",
    "- **LangChain solves these problems by:**\n",
    "  - Providing structured classes (`PromptTemplate`, `ChatPromptTemplate`)\n",
    "  - Adding validation and error-checking\n",
    "  - Enabling reusability (save/load templates)\n",
    "  - Integrating prompts seamlessly with chains and workflows\n",
    "  - Supporting advanced features like role-based messages and dynamic chat history\n",
    "- **Most reasonable reason to use LangChain:**  \n",
    "  It transforms *ad-hoc experimentation* into *production-ready workflows* with clarity, consistency, and context management.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. LangChain Prompt Types \n",
    "- **Static Prompts** ‚Üí Hardcoded text (simple, but error-prone)\n",
    "- **Dynamic Prompts** ‚Üí Templates with placeholders (flexible, consistent)\n",
    "- **PromptTemplate Class** ‚Üí Validation, reusability, integration\n",
    "- **ChatPromptTemplate** ‚Üí Multi-turn conversations with role-based messages\n",
    "- **Message Types**:\n",
    "  - SystemMessage ‚Üí sets assistant‚Äôs role/context\n",
    "  - HumanMessage ‚Üí user input\n",
    "  - AIMessage ‚Üí model response\n",
    "- **MessagePlaceholder** ‚Üí Insert chat history dynamically at runtime\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c61c02",
   "metadata": {},
   "source": [
    "# Vanilla Prompt Styles in GenAI\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Instruction Prompts\n",
    "- **Definition:** Direct commands or tasks given to the model.\n",
    "- **How it works:** The LLM interprets the instruction and produces output accordingly.\n",
    "- **Example:** \"Summarize this article in 3 bullet points.\"\n",
    "- **Use case:** Q&A, summarization, translation, content generation.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Zero-Shot Prompts\n",
    "- **Definition:** Asking the model to perform a task without providing examples.\n",
    "- **How it works:** Relies entirely on the model‚Äôs pretraining and general knowledge.\n",
    "- **Example:** \"Classify the sentiment of this sentence: 'I love this product.'\"\n",
    "- **Use case:** Quick tasks where the model already knows the concept.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Few-Shot Prompts\n",
    "- **Definition:** Providing a few examples before the actual query.\n",
    "- **How it works:** The model learns the pattern from examples and applies it to the new input.\n",
    "- **Example:**\n",
    "\n",
    "### Text: \"I am happy.\" ‚Üí Sentiment: Positive\n",
    "\n",
    "    Text: \"I hate this.\" ‚Üí Sentiment: Negative\n",
    "    Text: \"This is okay.\" ‚Üí Sentiment:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9e85ae",
   "metadata": {},
   "source": [
    "- **Use case:** Classification, formatting, or tasks needing consistency.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Chain-of-Thought Prompts\n",
    "- **Definition:** Prompts that encourage the model to reason step by step.\n",
    "- **How it works:** The LLM generates intermediate reasoning before giving the final answer.\n",
    "- **Example:** \"Solve this math problem step by step: 23 √ó 17\"\n",
    "- **Use case:** Logical reasoning, math problems, multi-step tasks.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Role-Based Prompts\n",
    "- **Definition:** Assigning a persona or role to the model.\n",
    "- **How it works:** The LLM adopts the role and tailors responses accordingly.\n",
    "- **Example:** \"You are a Python tutor. Explain recursion with a simple code example.\"\n",
    "- **Use case:** Teaching, customer support, creative writing with tone control.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Contextual Prompts\n",
    "- **Definition:** Providing background information before asking the question.\n",
    "- **How it works:** The LLM uses the given context to generate relevant answers.\n",
    "- **Example:** \"Here is a paragraph about climate change. Based on it, write a short quiz question.\"\n",
    "- **Use case:** Context-aware Q&A, grounded responses.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Conversational Prompts\n",
    "- **Definition:** Simulating dialogue with alternating user/assistant turns.\n",
    "- **How it works:** The LLM continues the conversation based on prior turns.\n",
    "- **Example:**\n",
    "\n",
    "### User: What is AI?\n",
    "\n",
    "Assistant: AI is the simulation of human intelligence by machines.\n",
    "\n",
    "User: Can you give me an example?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e499302e",
   "metadata": {},
   "source": [
    "- **Use case:** Chatbots, interactive assistants.\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Multimodal Prompts\n",
    "- **Definition:** Prompts that include text + other input types (image, audio, video).\n",
    "- **How it works:** The LLM processes multiple modalities to generate output.\n",
    "- **Example:** \"Describe the image and suggest a caption.\"\n",
    "- **Use case:** Vision-language tasks, audio transcription, video summarization.\n",
    "\n",
    "---\n",
    "\n",
    "## üîë Key Insight\n",
    "- Vanilla prompts = **flexible and creative** starting point.\n",
    "- They show how LLMs respond to different styles of input.\n",
    "- Later, frameworks like **LangChain** add structure, validation, and reusability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66947794",
   "metadata": {},
   "source": [
    "# üîë Setting up GROQ API Key\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Create a `.env` file\n",
    "In your project root folder, create a file named `.env` and add:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bd2d6b",
   "metadata": {},
   "source": [
    "\n",
    "‚ö†Ô∏è **Best Practice:**  \n",
    "- Add `.env` to `.gitignore` so secrets aren‚Äôt pushed to GitHub.  \n",
    "- Keep API keys private.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Load the API Key in Python\n",
    "We use `python-dotenv` to read the `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c97b43b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting groq\n",
      "  Downloading groq-1.0.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\admin\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from groq) (4.12.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\admin\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\admin\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from groq) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\admin\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from groq) (2.12.5)\n",
      "Requirement already satisfied: sniffio in c:\\users\\admin\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in c:\\users\\admin\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from groq) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\admin\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from anyio<5,>=3.5.0->groq) (3.11)\n",
      "Requirement already satisfied: certifi in c:\\users\\admin\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\admin\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\admin\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\admin\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\admin\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\admin\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (0.4.2)\n",
      "Downloading groq-1.0.0-py3-none-any.whl (138 kB)\n",
      "Installing collected packages: groq\n",
      "Successfully installed groq-1.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.3 -> 26.0.1\n",
      "[notice] To update, run: C:\\Users\\admin\\AppData\\Local\\Python\\pythoncore-3.14-64\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cec9c620",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from groq import Groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "497f345c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7147d9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the GROQ API key\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ee35aafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize client\n",
    "client = Groq(api_key=groq_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "34a3885a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded GROQ API Key: gsk_C...\n"
     ]
    }
   ],
   "source": [
    "# Print to verify (optional, avoid in production)\n",
    "print(\"Loaded GROQ API Key:\", groq_api_key[:5] + \"...\" if groq_api_key else \"Not Found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9dd559",
   "metadata": {},
   "source": [
    "# üìù Instruction Prompts\n",
    "\n",
    "---\n",
    "\n",
    "## Definition\n",
    "Direct commands or tasks given to the model.\n",
    "\n",
    "---\n",
    "\n",
    "## How it Works\n",
    "- The LLM interprets the instruction literally.\n",
    "- Produces output directly aligned with the command.\n",
    "- No examples or context are required ‚Äî just the instruction itself.\n",
    "\n",
    "---\n",
    "\n",
    "## Example\n",
    "- \"Summarize this article in 3 bullet points.\"\n",
    "- \"Translate the following sentence into French.\"\n",
    "- \"Write a short poem about cricket.\"\n",
    "\n",
    "---\n",
    "\n",
    "## Use Cases\n",
    "- Q&A systems\n",
    "- Summarization\n",
    "- Translation\n",
    "- Content generation\n",
    "- Quick factual or creative tasks\n",
    "\n",
    "---\n",
    "\n",
    "## Hands-On Exercise (with GROQ)\n",
    "We‚Äôll use the GROQ API to send a simple instruction prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d5847613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send Instructions prompt\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model = \"llama-3.1-8b-instant\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Summarize the importance of AI in 3 bullet points.\"}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bcd3e434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a summary of the importance of AI in 3 bullet points:\n",
      "\n",
      "* **Improved Efficiency and Productivity**: Artificial Intelligence (AI) automates tasks, freeing up human resources to focus on complex and high-value tasks. This leads to increased productivity and efficiency in various industries, including healthcare, finance, and manufacturing.\n",
      "* **Enhanced Decision-Making and Insights**: AI systems can process vast amounts of data, identify patterns, and provide valuable insights that humans may miss. This enables organizations to make informed decisions, predict trends, and mitigate risks, leading to better business outcomes.\n",
      "* **Personalized Experiences and Improved Customer Service**: AI-powered systems can analyze customer behavior, preferences, and habits, enabling tailored products, services, and experiences. This leads to enhanced customer satisfaction, loyalty, and retention, ultimately driving business growth and revenue.\n"
     ]
    }
   ],
   "source": [
    "# print the output\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b207f7b",
   "metadata": {},
   "source": [
    "# üìù Instruction Prompts ‚Äì Hands-On\n",
    "\n",
    "---\n",
    "\n",
    "## What We Write\n",
    "- We give the model a **direct command** or task.\n",
    "- Example: `\"Summarize the importance of AI in 3 bullet points.\"`\n",
    "- This is plain text, no examples or context ‚Äî just the instruction.\n",
    "\n",
    "---\n",
    "\n",
    "## How It Works with the Model\n",
    "- The LLM reads the instruction as the **user‚Äôs intent**.\n",
    "- It generates output aligned with the command:\n",
    "  - If asked to summarize ‚Üí it condenses information.\n",
    "  - If asked to translate ‚Üí it converts text into another language.\n",
    "  - If asked to write ‚Üí it produces creative or factual text.\n",
    "\n",
    "---\n",
    "\n",
    "## Code Example (with GROQ)\n",
    "```python\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",   # fast model for demo\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Summarize the importance of AI in 3 bullet points.\"}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da231be",
   "metadata": {},
   "source": [
    "# üì¶ Understanding the Response\n",
    "\n",
    "---\n",
    "\n",
    "The API returns a **response object** with multiple fields.  \n",
    "The actual text generated by the model is inside:\n",
    "\n",
    "```python\n",
    "response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ff152766",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-51977323-89b4-43e6-96fb-6cede31d1958', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Here's a summary of the importance of AI in 3 bullet points:\\n\\n* **Improved Efficiency and Productivity**: Artificial Intelligence (AI) automates tasks, freeing up human resources to focus on complex and high-value tasks. This leads to increased productivity and efficiency in various industries, including healthcare, finance, and manufacturing.\\n* **Enhanced Decision-Making and Insights**: AI systems can process vast amounts of data, identify patterns, and provide valuable insights that humans may miss. This enables organizations to make informed decisions, predict trends, and mitigate risks, leading to better business outcomes.\\n* **Personalized Experiences and Improved Customer Service**: AI-powered systems can analyze customer behavior, preferences, and habits, enabling tailored products, services, and experiences. This leads to enhanced customer satisfaction, loyalty, and retention, ultimately driving business growth and revenue.\", role='assistant', annotations=None, executed_tools=None, function_call=None, reasoning=None, tool_calls=None))], created=1770883670, model='llama-3.1-8b-instant', object='chat.completion', mcp_list_tools=None, service_tier='on_demand', system_fingerprint='fp_d317489708', usage=CompletionUsage(completion_tokens=169, prompt_tokens=48, total_tokens=217, completion_time=0.238852409, completion_tokens_details=None, prompt_time=0.006126251, prompt_tokens_details=None, queue_time=0.333236595, total_time=0.24497866), usage_breakdown=None, x_groq=XGroq(id='req_01kh8e8r3neke9m6vhk3acjerj', debug=None, seed=1550514446, usage=None))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bdbaac",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Breakdown\n",
    "- **`response.choices`** ‚Üí a list of possible outputs (here it contains 1 `Choice` object).\n",
    "- **`[0]`** ‚Üí selects the first choice in the list.\n",
    "- **`.message`** ‚Üí the message object returned by the model.\n",
    "- **`.content`** ‚Üí the actual text string generated by the LLM.\n",
    "\n",
    "---\n",
    "\n",
    "### Why `[0]`?\n",
    "- The API can return **multiple choices** if requested.  \n",
    "- By default, it returns **one choice**, stored at index `0`.  \n",
    "- That‚Äôs why we write:\n",
    "\n",
    "```python\n",
    "response.choices[0].message.content ## to extract the generated text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953c2a91",
   "metadata": {},
   "source": [
    "< output >\n",
    "\n",
    "Here's a summary of the importance of AI in 3 bullet points:\n",
    "\n",
    "* Improved Efficiency and Productivity\n",
    "* Enhanced Decision-Making and Insights\n",
    "* Personalized Experiences and Improved Customer Service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df8e14d",
   "metadata": {},
   "source": [
    "## Key Insight: \n",
    "\n",
    "    --The model‚Äôs output is always inside .choices[0].message.content.\n",
    "    --If multiple choices are requested, you can loop through response.choices to print all outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3e20af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
